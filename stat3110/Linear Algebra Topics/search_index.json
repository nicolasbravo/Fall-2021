[["linear-algebra-introduction.html", "Linear Algebra Topics 1 Linear Algebra Introduction 1.1 Scalar 1.2 Vector 1.3 Basic Functions 1.4 Vector Multiplication 1.5 Vector Space", " Linear Algebra Topics Nico Bravo 11/10/2021 1 Linear Algebra Introduction 1.1 Scalar Represented in this class by a lowercase greek letter A single number: “constant” For example, 2 is a scalar, \\(\\pi\\) is a scalar, etc 1.2 Vector Represented in this class by a lowercase letter \\(v = (v_1, v_2, v_3)\\) A collection of numbers Line of dimension \\(p\\) determined by a magnitude and direction The direction is detailed by the values in the vector \\((1,-2)\\) would be \\(1\\) unit in the \\(x\\) direction and \\(-2\\) units in the \\(y\\) direction The magnitude essentially is the length from the head of the vector to the tail of the vector \\(||(a,b,c)|| = \\sqrt{a^2 + b^2 + c^2}\\) Ways to write Parenthesis: \\((1, -2)\\) Can also be stacked: \\(\\binom{1}{-2}\\) Brackets: \\([1,-2]\\) Angled brackets: \\(&lt;1,-2&gt;\\) Row vectors: \\[ v=(v_1,v_2,\\dots,v_p) \\] - Column vectors: \\[ v = \\begin{pmatrix} v_1\\\\ v_2\\\\ \\vdots\\\\ v_p \\end{pmatrix} \\] Note that sometimes it is easier to write column vectors horizontally by using the Transpose function covered later. This would be done by using \\(v = (v_1,v_2,\\dots,v_p)^T\\). 1.2.1 Zero Vector \\[ 0_p = (0,0,\\dots,0)^T \\] 1.2.2 One Vector \\[ 1_p = (1,1,\\dots,1)^T \\] 1.2.3 Unit Vector This is a vector of any direction with a magnitude of 1. \\[ u = (u,u,\\dots,u)^T, ||u|| = 1 \\] Note that if we multiply a unit vector by \\(\\lambda\\), the resulting magnitude would be \\(||\\lambda u|| = \\lambda (1) = \\lambda\\). This implies that we can make a unit vector out of any vector. To do so, we would simply need to multiply the vector by one over the magnitude. \\[ \\frac{1}{||v||} v = \\begin{pmatrix} v_1 / ||v||\\\\ v_2 / ||v||\\\\ v_3 / ||v|| \\end{pmatrix} \\] To denote these unit vectors in a specified direction, we would use a hat. For example, this previous vector \\(v\\)’s unit vector would be designated as \\(\\hat{v}\\). Let’s try this out in an example. Say our vector \\(v = (1, -2, 4)^T\\). Our magnitude would therefore be \\[ ||v|| = \\sqrt{(1)^2 + (-2)^2 + (4)^2} = \\sqrt{1+4+16} = \\sqrt{21} \\] Our unit vector \\(\\hat{v}\\) would then be \\[ \\hat{v} = \\left( \\frac{1}{\\sqrt{21}}, \\frac{-2}{\\sqrt{21}}, \\frac{4}{\\sqrt{21}} \\right)^T \\] Where \\(|| \\hat{v} || = 1\\). Another thing to note is that the dot product between a unit vector and itself (its transpose) is 1: \\(u^Tu = 1\\). 1.2.4 Row and Column Vectors A column vector has \\(p\\) rows, for dimension \\(p \\times 1\\) A row vector has \\(p\\) columns, for dimension \\(1 \\times p\\) 1.3 Basic Functions 1.3.1 Transpose This will flip the rows and columns (turn a row vector into a column vector, or the other way around). The transpose of a vector is denoted by a \\(T\\) or apostrophe, for example \\(v \\Rightarrow v^T\\) or \\(v&#39;\\). 1.3.2 Addition or Subtraction Assume we are talking about a vector \\(a\\) with three elements: \\[ a= \\begin{pmatrix} a_1\\\\ a_2\\\\ a_3 \\end{pmatrix} \\] And a vector \\(b\\) with three elements: \\[ b = \\begin{pmatrix} b_1\\\\ b_2\\\\ b_3 \\end{pmatrix} \\] If we were to add the vectors together, it would look something like this: \\[ a+b = \\begin{pmatrix} a_1 + b_1\\\\ a_2 + b_2\\\\ a_3 + b_3 \\end{pmatrix} = b+a \\] Note that addition is commutative. If we were to instead subtract, it would work the exact same way, except that subtraction is not commutative for vectors. Graphically, addition appears as the addition of two lines, while subtraction appears as the subtraction of two lines (view an image for this, as this description I gave was not as clear as I’d wish I could make it). Also, for addition or subtraction to work, the dimensions of the two vectors must be the same. This also results in the vector created being the same dimension-wise. Let’s look at an example. Say we have vectors \\(a = (1,0,-3)^T\\) and \\(b = (-2,4,1)^T\\). Let’s first perform addition: \\[ a+b= \\begin{pmatrix} 1+-2\\\\ 0+4\\\\ -3+1 \\end{pmatrix} = \\begin{pmatrix} -1\\\\ 4\\\\ -2 \\end{pmatrix} \\] Now, let’s try to perform subtraction: \\[ a+b= \\begin{pmatrix} 1--2\\\\ 0-4\\\\ -3-1 \\end{pmatrix} = \\begin{pmatrix} -3\\\\ -4\\\\ -4 \\end{pmatrix} \\] 1.3.3 Vector-Scalar Multiplication Let’s say we have a vector \\(v = (v_1, v_2, v_3)^T\\) and a scalar value \\(\\lambda\\). If we were multiplying this vector by the scalar, we would multiply each member of the vector by the scalar value: \\[ \\lambda v = (\\lambda v_1, \\lambda v_2, \\lambda v_3)^T \\] The effect of this is that the magnitude of the vector will change. The direction will not change. If \\(\\lambda &gt; 1\\), \\(v\\) will be stretched \\(\\lambda\\) times longer than the original If \\(0 &lt; \\lambda &lt; 1\\), \\(v\\) will be shrunk by a factor of \\(\\lambda\\) If \\(-1 &lt; \\lambda &lt; 0\\), \\(v\\) will be shrunk in the opposite direction by a factor of \\(\\lambda\\). If \\(\\lambda &lt; -1\\), \\(v\\) will be stretched in the opposite direction \\(\\lambda\\) times longer than the original If \\(\\lambda = 0\\), \\(v\\) is reduced to a point with zero magnitude, known as a “zero vector” Let’s try some examples. Assume we have a vector \\(v = (1,-2,4)^T\\). \\(\\lambda = 2\\) \\[ 2v = (2,-4,8)^T \\] \\(\\lambda = \\frac{-1}{2}\\) \\[ \\frac{-1}{2}v = \\left(\\frac{-1}{2},1,-2\\right)^T \\] \\(\\lambda = 0\\) \\[ 0v = 0_3 = (0,0,0)^T \\] 1.4 Vector Multiplication 1.4.1 Vector Dot Product The sum of the products of corresponding entries Produces a scalar Both vectors must have same length Dot product between a vector and itself is the magnitude squared If \\(a\\) and \\(b\\) are \\(p \\times 1\\) vectors, \\[ a \\cdot b = a^T b = \\sum_{i=1}^p a_i b_i \\] Properties: - Dot product is invariant to associations of scalar-vector multiplication - \\(\\lambda (a \\cdot b) = (\\lambda a) \\cdot b = a \\cdot (\\lambda b) = (a \\cdot b) \\lambda\\) - BUT… The dot product is not associative in general - \\(a (b \\cdot c) \\neq (a \\cdot b)c\\) - Commutative Property - YES! \\(a \\cdot b = b \\cdot a\\) - Distributive Property - YES! \\(a \\cdot (b + c) = a \\cdot b + a \\cdot c\\) - Cauchy-Schwarz Inequality - Absolute value of the dot product is less than or equal to product of magnitudes - \\(|a^Tb| \\leq ||a||||b||\\) 1.4.2 Angle Between Vectors We can use the dot product to solve for the angle between the two vectors: \\[\\begin{align*} a \\cdot b = a^Tb &amp;= ||a||||b||\\cos (\\theta_{ab})\\\\ \\cos(\\theta_{ab}) &amp;= \\frac{a^Tb}{||a||||b||}\\\\ \\theta_{ab} &amp;= \\cos^{-1} \\left( \\frac{a^Tb}{||a||||b||} \\right) \\end{align*}\\] Note that if \\(\\theta = 0\\), then \\(a^Tb = ||a||||b||\\) (if \\(\\theta = 180\\), then \\(a^Tb = -||a||||b||\\)). 1.4.3 Linear Weighted Combination This is a weighted combination of vectors (where \\(\\lambda_i\\) are scalars and \\(v_i\\) are vectors of equal dimension): \\[ w = \\lambda_1 v_1 + \\lambda_2 v_2 + \\cdots + \\lambda_p v_p \\] 1.4.4 Outer Product This is a vector multiplication that results in a matrix. While the dot product sums the products, this puts each product as a value in a matrix. \\[ ab^T = \\begin{pmatrix} a_1b_1&amp;a_1b_2&amp;\\cdots&amp;a_1b_p\\\\ a_2b_1&amp;a_2b_2&amp;\\cdots&amp;a_2b_p\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ a_pb_1&amp;a_pb&amp;\\cdots&amp;a_pb_p \\end{pmatrix} \\] While a dot product is only valid for vectors of equal dimension, the outer product is valid for vectors of any dimension. It makes a matrix of size \\((M \\times N)\\), if the dimensions of \\(a\\) and \\(b\\) are \\((M \\times 1)\\) and \\((1 \\times N)\\), respectively. 1.5 Vector Space 1.5.1 Definition of a Field A field is a set of numbers for which the basic arithmetic operations are defined (ex. \\(\\mathbb{R}\\), \\(\\mathbb{Z}\\), \\(\\mathbb{N}\\), etc). For vectors \\(a\\) and \\(b\\), where \\(a = (a_1,a_2,\\cdots,a_m)^T\\) \\(b = (b_1,b_2,\\cdots,b_n)^T\\) These vectors are defined as \\(a\\) is in the space of all real numbers to the power of the dimension \\(a \\in \\mathbb{R}^m\\) Same with \\(b\\). \\(b \\in \\mathbb{R}^n\\) 1.5.2 Vector Space Any set of objects for which addition and scalar multiplication are defined Additive Inverse Associativity Commutativity Additive Identity Multiplicative Identity Distributivity 1.5.3 Subspace All other vectors that can be obtained from this vector Any value of \\(\\lambda\\) times the vector for \\(\\lambda \\in \\mathbb{R}\\) Any linear combination we can obtain from our original set is in the subset The set of all points obtained by combining a collection of vectors through addition and scalar multiplication Some space that is “spanned” by a set of vectors If \\(a = (-1,2)^T\\) and \\(b = (1,0)^T\\), For one vector, a subspace is a line: \\(\\lambda_1 a\\) is a subspace for any \\(\\lambda_1 \\in \\mathbb{R}\\) This vector subspace would be all of the vectors obtained by multiplying \\(a\\) by the constant \\(\\lambda_1\\) For two vectors, a subspace is a plane: \\(\\lambda_1 a + \\lambda_2 b\\) is a subspace for any \\(\\lambda_1,\\lambda_2 \\in \\mathbb{R}\\) Any vector that can be created from the linear combination between the two vectors Any subspace must contain the origin for \\(\\lambda = 0\\) 1.5.4 Ambient space The space in which a subspace is embedded Has the same dimension as the vectors themselves (is the dimension of some vector) If dimension two, then \\(\\mathbb{R}^2\\) If dimension three, then \\(\\mathbb{R}^3\\) How many dimensions you need to explain a vector 1.5.5 Linear Independence One vector in a set is not a linear scaling of the others 1.5.6 Subset A region in space that can be bounded and doesn’t need to include the origin A subspace is a subset, but a subset is not a subspace necessarily Can be an inequality, while a subspace needs to be closed 1.5.7 Span The set of all points that can be obtained by any linear weighted combination of vectors The subspace of a set of vectors The span of a set of vectors is the subspace created by the vectors 1.5.8 Subspace Example \\[\\begin{align*} a = \\begin{bmatrix} 1\\\\0 \\end{bmatrix}, b = \\begin{bmatrix} 0\\\\1 \\end{bmatrix}, \\lambda_1 a + \\lambda_2 b = \\begin{bmatrix} \\lambda_1\\\\\\lambda_2 \\end{bmatrix}\\\\ span \\left\\{ \\begin{bmatrix}1\\\\0\\end{bmatrix},\\begin{bmatrix}0\\\\1\\end{bmatrix}\\right\\} = \\mathbb{R}^2 \\end{align*}\\] 1.5.9 Ambient Space Example \\[\\begin{align*} a = \\begin{bmatrix} 2\\\\1 \\end{bmatrix}, b = \\begin{bmatrix} 1\\\\1/2 \\end{bmatrix}, \\lambda_1 a + \\lambda_2 b = \\lambda a = \\lambda \\begin{bmatrix}2\\\\1 \\end{bmatrix} = \\frac{1}{2}a\\\\ a \\in \\mathbb{R}^2, \\begin{bmatrix}v_1\\\\\\vdots\\\\v_n\\end{bmatrix} \\in \\mathbb{R}^n \\end{align*}\\] 1.5.10 Linear Dependence Say we have \\(n\\) vectors that are of size \\(m \\times 1\\) (\\(m\\) rows, 1 column). They are linearly dependent if \\(\\sum \\lambda_i v_i = 0\\). Our linear equation is \\[ \\lambda_1 v_1 + \\lambda_2 v_2 + \\dots + \\lambda_n v_n \\] For example, let’s try this with three vectors: \\[\\begin{align*} v_1 = \\begin{bmatrix}1\\\\0\\end{bmatrix}, v_2 = \\begin{bmatrix}2\\\\0\\end{bmatrix}, v_3 = \\begin{bmatrix}0\\\\2\\end{bmatrix}\\\\ 2v_1 - v_2 = 0 \\end{align*}\\] \\(v_1\\) and \\(v_2\\) are linearly dependent. 1.5.11 Determining Dependence Count number of vectors and the dimension If \\(n &gt; m\\), then dependent If \\(n \\leq m\\), chance they are not dependent Check for a zero vector If one exists, then dependent Comparing linear combinations 1.5.12 Basis Some set of linearly independent vectors that span the subspace. Think of it as the opposite of the subspace, where instead of getting the linear equation, we get the end result and work backwards. "],["matrix.html", "2 Matrix 2.1 Shifting 2.2 Addition 2.3 Scalar Multiplication 2.4 Matrix Multiplication 2.5 Identify Matrix 2.6 Inverse Matrix 2.7 Matrix Transposing 2.8 Symmetric Matrix 2.9 Matrix Norm 2.10 Matrix Rank", " 2 Matrix Say we have a matrix \\(A\\), with rows \\(m\\) and columns \\(n\\) \\((m \\times n)\\). THat would look something like: \\[ A = \\begin{bmatrix} a_{11}&amp;a_{12}&amp;\\cdots&amp;a_{1n}\\\\ a_{21}&amp;a_{22}&amp;\\cdots&amp;a_{2n}\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ a_{m1}&amp;a_{m2}&amp;\\cdots&amp;a_{mn} \\end{bmatrix} \\] 2.1 Shifting 2.1.1 Diagonal Matrix This kind of square matrix only has values on the diagonal. The rest of the values are 0. \\[ A = \\begin{bmatrix} a_{11}&amp;0&amp;\\cdots&amp;0\\\\ 0&amp;a_{22}&amp;\\cdots&amp;0\\\\ \\vdots&amp;\\vdots&amp;\\ddots&amp;\\vdots\\\\ 0&amp;0&amp;\\cdots&amp;a_{mn} \\end{bmatrix} \\] 2.1.2 Shifting Definition and Example We can “shift” the matrix by applying an equation such as \\(A + \\lambda I\\). This moves the entries by some specified value. This can turn linearly dependent columns into linearly independent columns. For example: \\[ A = \\begin{bmatrix} 1&amp;1&amp;2\\\\ 0&amp;0&amp;5\\\\ 0&amp;0&amp;3 \\end{bmatrix} + \\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1 \\end{bmatrix} = \\begin{bmatrix} 2&amp;1&amp;2\\\\ 0&amp;1&amp;5\\\\ 0&amp;0&amp;4 \\end{bmatrix} \\] By shifting the values a small amount, it forces the values to become independent. 2.1.3 Applying Diagonal If we take \\(diag(A)\\), it might generate a vector \\(v\\) containing the diagonal of the matrix \\(A\\). Note that if the matrix is not a square, the diagonal is the values starting from the top-leftmost position going down and right until it hits an end. In this case, the diagonal vector’s size is \\(\\min (m,n)\\). 2.1.4 Trace The trace is the sum of diagonal elements It is only defined for square matrices. \\[ trace \\left( \\begin{bmatrix}5&amp;3&amp;8\\\\1&amp;0&amp;4\\\\8&amp;6&amp;2\\end{bmatrix}\\right) = 5 + 0 + 2 = 7 \\] 2.2 Addition We can simply add just like vector addition and subtraction. \\[ A + B = \\begin{bmatrix} a_{11}+b_{11}&amp;\\cdots&amp;a_{1n}+b_{1n}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ a_{m1}+b_{m1}&amp;\\cdots&amp;a_{mn}+b_{mn} \\end{bmatrix} \\] 2.3 Scalar Multiplication This is commutative and just like vectors. Because of this, \\(\\lambda AB = A \\lambda B = AB \\lambda\\). \\[ \\lambda A = \\begin{bmatrix} \\lambda a_{11}&amp;\\cdots&amp;\\lambda a_{1n}\\\\ \\vdots&amp;\\ddots&amp;\\vdots\\\\ \\lambda a_{m1}&amp;\\cdots&amp;\\lambda a_{mn} \\end{bmatrix} \\] 2.4 Matrix Multiplication \\(AB \\neq BA\\) because they do not have the same number of rows and columns. Note that \\(A(B + C) = AB + AC\\), so we can distribute Because not commutative, \\(A(B + C) \\neq (B+C)A\\). \\((B+C)A = BA + CA\\) Let’s say we have matrix \\(A\\) with dimensions (\\(N \\times M\\)) and matrix \\(B\\) with dimensions (\\(K \\times L\\)). If \\(M = K\\) (if the inner values are the same), then matrix multiplication is valid and will generate a matrix with dimensions (\\(N \\times L\\)). 2.4.1 How do we multiply them? Let’s say we want to find \\(AB\\). For each element \\(ab_{ij}\\) where \\(i\\) is some row and \\(j\\) is some column, we would take the dot product between row \\(i\\) of matrix \\(A\\) and column \\(j\\) of matrix \\(B\\). For example, value \\(ab_{53}\\) (value \\(ab\\) at row 5 and column 3) would be equal to the dot product between row 5 of matrix \\(A\\) and column 3 of matrix \\(B\\). 2.4.2 Matrix Multiplication Example, (2x2) \\[\\begin{align*} A &amp;= \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21}&amp;a_{22} \\end{bmatrix}\\\\ B &amp;= \\begin{bmatrix} b_{11}&amp;b_{12}\\\\ b_{21}&amp;b_{22} \\end{bmatrix}\\\\ AB &amp;= \\begin{bmatrix} a_{11}b_{11} + a_{12}b_{21} &amp; a_{11}b_{12} + a_{12}b_{22}\\\\ a_{21}b_{11} + a_{22}b_{21} &amp; a_{21}b_{12} + a_{22}b_{22} \\end{bmatrix} \\end{align*}\\] 2.4.3 How does this relate to the dot product? Say we have two vectors, \\(a\\) with dimensions \\((n \\times 1)\\), and \\(b\\) with dimensions \\((n \\times 1)\\). In this case, \\[ ab = a^Tb = \\sum_{i=1}^n a_i b_i \\] This would generate a number. In a similar case, we could do matrix multiplication on \\(a^T\\) and \\(b\\) to generate the same number, as this would create a matrix with dimensions \\((1 \\times 1)\\). 2.5 Identify Matrix Square, diagonal matrix where all diagonal entries are 1 The reason this is called an identity matrix is because if you multiply a matrix by an identity matrix, then it returns exactly the matrix. \\(A_{(m \\times n)} I_n = A\\) \\(I_m A_{(m \\times n)} = A\\) \\[ I_3 = \\begin{bmatrix} 1&amp;0&amp;0\\\\ 0&amp;1&amp;0\\\\ 0&amp;0&amp;1 \\end{bmatrix} \\] 2.5.1 What does this mean for diagonal matrix multiplication? Because a diagonal matrix is just a scaled identity matrix, using matrix multiplication with some matrix and a diagonal matrix would make the matrix scale. Take for example: \\[ D = \\begin{bmatrix} d_1&amp;0&amp;0\\\\ 0&amp;d_2&amp;0\\\\ 0&amp;0&amp;d_3 \\end{bmatrix} \\] In this case, \\(AD\\) scales the columns of \\(A\\) \\(DA\\) scales the rows of \\(A\\) 2.5.2 Multiplying two diagonal matrices What is special in this case is that we can just multiply the corresponding elements of the diagonal matrices to create another diagonal matrix. \\[ \\begin{bmatrix} a_1&amp;0&amp;0\\\\ 0&amp;a_2&amp;0\\\\ 0&amp;0&amp;a_3 \\end{bmatrix} \\begin{bmatrix} b_1&amp;0&amp;0\\\\ 0&amp;b_2&amp;0\\\\ 0&amp;0&amp;b_3 \\end{bmatrix} = \\begin{bmatrix} a_1b_1&amp;0&amp;0\\\\ 0&amp;a_2b_2&amp;0\\\\ 0&amp;0&amp;a_3b_3 \\end{bmatrix} \\] 2.6 Inverse Matrix Matrix Division would essentially be multiplying by the inverse of a matrix (ex. \\(A^{-1}\\)). This is found because: \\[\\begin{align*} A &amp;= IA\\\\ (A^{-1})A &amp;= IA (A^{-1}) \\end{align*}\\] 2.6.1 Expanding Just like transposing, we can expand through with inverses, such as: \\((AB)^{-1} = B^{-1}A^{-1}\\). Note the flipping. Note that we can do this for more than two matrices Ex. \\((ABCD)^T = D^TC^TB^TA^T\\) 2.7 Matrix Transposing \\[\\begin{align*} A &amp;= \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21}&amp;a_{22} \\end{bmatrix}\\\\ A^T &amp;= \\begin{bmatrix} a_{11}&amp;a_{21}\\\\ a_{12}&amp;a_{22} \\end{bmatrix} \\end{align*}\\] 2.7.1 Expanding Something to note is that \\((AB)^T = B^T A^T\\). Note the flipping. Note that this can also be done with addition/subtraction. 2.8 Symmetric Matrix A symmetric matrix is one which is equal to its transpose (\\(A = A^T\\)). It is symmetric along its diagonal, as in the diagonal can be whatever but the other values must be mirrored. For example: \\[ \\begin{bmatrix} 0&amp;a&amp;b\\\\ a&amp;0&amp;c\\\\ b&amp;c&amp;0 \\end{bmatrix} \\] 2.8.1 Creating with addition A way to get a symmetric matrix is to add a square matrix to its transpose (\\(S^T + S = S + S^T =\\) symmetric matrix). We can prove this: \\[\\begin{align*} (S+S^T)^T &amp;= S^T + S^{TT}\\\\ &amp;= S^T + S\\\\ &amp;= S + S^T \\end{align*}\\] Note that the transpose can be expanded to addition as well. 2.8.2 Creating with multiplication We can also make a symmetric matrix by multiplying a matrix by its transpose in any order. \\[\\begin{align*} BB^T &amp;\\Rightarrow \\text{ symmetric}\\\\ B^TB &amp;\\Rightarrow \\text{ symmetric} \\end{align*}\\] 2.9 Matrix Norm This is a scalar value that quantifies the magnitude of a matrix. 2.9.1 Frobenius Norm Also called an \\(\\ell_2\\)-norm, it is the most common norm and is denoted by \\(||A||_F\\) (do not use subscript 2, that’s something else). It is equal to: \\[ ||A||_F = \\sqrt{\\sum_{i=1}^M \\sum_{j=1}^N a_{ij}^2} \\] If we had some matrix \\(A\\) as follows: \\[ A = \\begin{bmatrix} a_{11}&amp;a_{12}\\\\ a_{21}&amp;a_{22} \\end{bmatrix} \\] The Frobenius norm would be: \\[ ||A||_F = \\sqrt{a_{11}^2 + a_{12}^2 + a_{21}^2 + a_{22}^2} \\] 2.9.2 \\(\\ell_1\\)-norm \\[ ||A||_1 = \\sum_{i=1}^M \\sum_{j=1}^N |a_{ij}| \\] 2.9.3 \\(\\ell_p\\)-norm \\[ ||A||_p = \\left( \\sum_{i=1}^M \\sum_{j=1}^N |a_{ij}|^p \\right)^{1/p} \\] 2.10 Matrix Rank This is the number of dimensions of information that the matrix contains This is essentially the number of linearly independent columns in a matrix The rank of any matrix is bounded between 0 and the minimum number of rows or columns in the matrix \\(0 \\leq rank(A) \\leq \\min(M,N)\\) The only time a rank is 0 is for a zero vector In any other case, the rank is \\(\\geq 1\\). A matrix is considered to be “full rank” if it is equal to \\(\\min(M,N)\\) If less than this value, it is considered “rank deficient” \\(rank(A) = rank(A^T)\\) 2.10.1 Rank of added matrices \\(rank(A \\pm B) \\leq rank(A) + rank(B)\\) For example, \\(A+B\\) makes a \\(3 \\times 3\\) matrix \\(rank(A) = 3\\) \\(rank(B) = 2\\) \\(\\therefore rank(A+B) = 3\\) 2.10.2 Rank of multiplied matrices \\(rank (AB) \\leq \\min (rank(A), rank(B))\\) For example, \\(AB\\) makes a \\(3 \\times 3\\) matrix \\(rank(A) = 3\\) \\(rank(B) = 2\\) \\(\\therefore rank(AB) = 2\\) "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
