[["other-continuous-distributions.html", "Unit 5: Other Continuous Distributions and Transformations 1 Other Continuous Distributions 1.1 Gamma Distribution 1.2 Exponential distribution 1.3 Beta Distribution 1.4 Section Summary", " Unit 5: Other Continuous Distributions and Transformations Nico Bravo 1 Other Continuous Distributions 1.0.1 Location Scale Family \\(\\Theta_1 =\\) location, \\(\\Theta_2=\\) scale \\[ F(x, \\Theta_1, \\Theta_2) = g(\\frac{x-\\Theta_1}{\\Theta_2}) \\] Think back to the cdf of the Uniform distribution, and replace \\(a\\) with \\(\\Theta_1\\) and \\(b-a\\) with \\(\\Theta_2\\): \\[ F(x) = \\frac{x-a}{b-a} = \\frac{x-\\Theta_1}{\\Theta_2} \\] The Normal distribution is also a location scale family, where this time \\(\\Theta_1 = \\mu\\) and \\(\\Theta_2 = \\sigma^2\\). \\[ F(x) = \\int_{-\\infty}^{x} \\frac{1}{\\sigma \\sqrt{2}} \\text{ exp } \\left( \\frac{-1}{2} \\cdot \\frac{(x-\\Theta_1)^2}{\\Theta_2} \\right) \\] 1.1 Gamma Distribution \\(X \\sim Gamma(\\alpha, \\beta)\\) \\(E(x) = \\alpha \\beta\\) \\(V(x) = \\alpha \\beta^2\\) \\(f(x) = \\frac{1}{\\beta^{\\alpha} \\Gamma (\\alpha)} x^{\\alpha-1}e^{-x/\\beta}\\) for \\(x &gt; 0\\) Note that when \\(\\alpha = 1\\), this becomes really easy to integrate (no need for integration by parts) \\(f(x) = \\frac{1}{\\beta} e^{-x/\\beta}\\) \\(F(x,\\alpha,\\beta) = F(\\frac{x}{\\beta},\\alpha,1)\\) to convert to standard gamma function for CDF “Standard Gamma” distribution/“Incomplete Gamma Function” \\(\\beta = 1\\) \\(f(x) = \\frac{1}{\\Gamma (\\alpha)} x^{\\alpha-1}e^{-x}\\) for \\(x &gt; 0\\) \\(F(x) = \\int_0^x \\frac{1}{\\Gamma(\\alpha)} x^{\\alpha-1}e^{-x}dx\\) Gamma function \\(\\Gamma(\\alpha)\\) \\(\\Gamma(\\alpha) = \\int_0^{\\infty} x^{\\alpha - 1}e^{-x}dx\\) We just need 3 rules to solve (rule 2 is the one we’ll see most in this class): For any \\(\\alpha &gt; 1\\), the Gamma function can be solved recursively: \\(\\Gamma(\\alpha) = (\\alpha - 1) \\Gamma(\\alpha - 1)\\) For any positive integer, \\(\\Gamma(\\alpha) = (\\alpha-1)!\\) \\(\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\) 1.2 Exponential distribution \\(X \\sim Exp(\\lambda)\\) \\(\\alpha = 1\\) \\(E(x) = \\frac{1}{\\lambda} = \\beta\\) \\(V(x) = \\frac{1}{\\lambda^2} = \\beta^2\\) \\(f(x) = \\lambda e^{-\\lambda x}\\) for \\(x &gt; 0\\) This is because all the \\(\\alpha\\) in the gamma distribution equal 1 and \\(\\frac{1}{\\beta} = \\lambda\\) \\(F(x) = \\int_0^x \\lambda e^{-\\lambda x} = 1 - e^{-\\lambda x}\\) for \\(x &gt; 0\\) As \\(\\lambda\\) increases, the spread decreases and the expected value gets closer to 0. 1.2.1 Special cases with the exponential distribution If \\(\\lambda = 1\\), \\[ \\sum_{i = 1}^{\\alpha} Exp(1) \\sim Gamma(\\alpha,1) \\] If we have \\(U\\) as a uniform distribution with 0 and 1 (\\(U \\sim Unif(0,1)\\)), \\[ -\\ln (U) \\sim Exp(1) \\] 1.2.2 Poisson Process A Poisson Process includes a Poisson distribution. We have event-times (times at which an event happens) \\(T_0, T_1, T_2, T_3, \\dots\\). Say we have a variable \\(Y =\\) number of events that occur over time \\(t\\). Then, we have \\(Y \\sim Poisson(\\alpha t)\\). \\(Y\\) has a pdf of: \\[ P(Y = y) = \\frac{e^{-\\alpha t} (\\alpha t)^y}{y^1} \\text{ for } y = 0,1,\\dots \\] In this scenario, \\(\\Delta T = T_1, T_2 - T_1, T_3 - T_2 = T_n - T_{n-1}\\) as the time progresses. These differences follow an exponential distribution \\(T_n - T_{n-1} \\sim Exp(\\alpha)\\). 1.2.3 Memoryless Property Assume \\(Y \\sim Exp(\\lambda)\\) where we are looking for the amount of time passing between cars appearing on a road. No time has passed in the first ten seconds - let’s call this \\(s\\). Let’s call \\(t\\) the next ten seconds. We want to calculate the probability that we would have to wait \\(t\\) more time after waiting \\(s\\). \\[ P(Y &gt; s + t | Y &gt; s) = P(Y &gt; t) \\] What does this mean? This means that because the exponential distribution is memoryless, we are just looking for the \\(P(Y &gt; t)\\), as \\(s\\) should not affect the probability. 1.2.4 Example Say we are looking for \\(X =\\) number of cars that drive by in some time \\(t\\), where \\(Y =\\) the time betwen two cars. \\(\\alpha =\\) 2 cars drive by per minute. This gives us \\(X \\sim Poisson(2t)\\) and \\(Y \\sim Exp(2)\\). What is the probability that no cars pass by in one minute? \\[\\begin{align*} P(Y = 0, t = 1) &amp;= \\frac{e^{-2(1)}[(2)(1)]^0}{0!}\\\\ &amp;= e^{-2} \\end{align*}\\] What is the probability that the time for a car to pass is less than 5 seconds? We want 5/60 instead of 5 to convert seconds to minutes. \\[\\begin{align*} P(Y &lt; 5 \\text{ sec}) &amp;= \\int_0^{5/60} 2e^{-2}dy\\\\ &amp;= 1 - e^{-2y} \\Bigr\\rvert_{y=0}^{y=5/60} \\end{align*}\\] What is the probability another car won’t come for more than 5 minutes, given a car hasn’t arrived in 2 minutes? Use the memoryless property. \\[ P(Y &gt; 5 | Y &gt; 3) = P(Y &gt; 2) \\] 1.3 Beta Distribution \\(E(x) = \\frac{\\alpha}{\\alpha + \\beta}\\) \\(V(x) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2 (\\alpha + \\beta + 1)}\\) \\(f(x) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha - 1}(1-x)^{\\beta-1}\\) Reminder that \\(\\Gamma(n) = (n-1)!\\) For this class, we are only going to work with the standard beta distribution, where \\(x \\in [0,1]\\) (instead of the window being \\(x \\in [a,b]\\).) 1.3.1 Special cases for the beta distribution When \\(\\alpha\\) and \\(\\beta\\) are equal to 1, the beta distribution is equal to a uniform distribution \\(Uniform(0,1)\\). 1.4 Section Summary 1.4.1 Uniform Distribution \\(X \\sim Unif(a,b)\\) PDF: \\[ f(x; a,b) = \\frac{1}{b-a} \\text{ for } x \\in [a,b] \\] Expected value: \\(E(X) = a + \\frac{1}{2} (b-a)\\) Variance: \\(V(X) = \\frac{1}{12} (b-a)^2\\) This is the continuous equivalent to “equally likely outcomes” Location scale family with \\(\\Theta_1 = a\\) and \\(\\Theta_2 = b-a\\) 1.4.2 Gamma Distribution \\(X \\sim Gamma(\\alpha, \\beta)\\) PDF: \\[ f(x; \\alpha, \\beta) = \\frac{1}{\\beta^{\\alpha} \\Gamma (\\alpha)} x^{\\alpha - 1}e^{-x / \\beta} \\text{ for } x&gt;0, \\alpha, \\beta&gt;0 \\] Expected value: \\(E(X) = \\alpha \\beta\\) Variance: \\(V(X) = \\alpha \\beta^2\\) A standard Gamma random variable has \\(\\beta = 1\\). You can get nonstandard probabilities from standard probabilities. The Gamma CDF for \\(\\alpha\\) and \\(\\beta\\) at \\(x=\\) Standard Gamma CDF for \\(\\alpha\\) at \\(x/\\beta\\) is as follows: \\[ F(x; \\alpha, \\beta) = F(\\frac{x}{\\beta}; \\alpha) \\] The rules to evaluating the gamma function are as follows: For any \\(\\alpha &gt; 1, \\Gamma(\\alpha) = (\\alpha - 1) \\Gamma(\\alpha -1)\\) For some positive integer, \\(\\alpha \\in \\mathbb{Z}^+, \\Gamma(\\alpha) = (\\alpha-1)!\\) \\(\\Gamma(\\frac{1}{2}) = \\sqrt{\\pi}\\) 1.4.3 Exponential Distribution \\(X \\sim Exp(\\lambda)\\) PDF: \\[ f(x; \\lambda) = \\lambda e^{-\\lambda x} \\text{ for } x &gt; 0, \\lambda &gt; 0 \\] Expected value: \\(E(X) = 1/\\lambda\\) Variance: \\(V(X) = 1/\\lambda^2\\) There are some properties that are important to know. \\(Gamma(\\alpha, 1) \\sim \\sum_{i=1}^{\\alpha} Exp(1)\\) If \\(U \\sim Unif(0,1)\\), then \\(-\\ln (U) \\sim Exp(1)\\) 1.4.4 Memoryless Property \\(P(X &gt;s + t | X &gt; t) = P(X &gt; s)\\). This means that the probability of something happening given that time has already past is the same as just the new time passing. 1.4.5 Poisson Process $X = $ number of arrivals in a time window of length \\(t\\) \\[ P_X(t) = \\frac{e^{-\\alpha t}(\\alpha t)^x}{x!} \\] Expected value: \\(E(X) = \\lambda = \\alpha t\\) \\(X = \\max(n : T_n \\leq t)\\) \\(n\\) is the number of arrivals observed before time \\(t\\) Times between events follow exponential distriutions: \\(T_n - T_{n-1} \\sim Exp(\\alpha)\\) 1.4.6 Beta Distribution \\(X \\sim Beta(\\alpha, \\beta)\\) PDF: \\[ f(x; \\alpha, \\beta) = \\frac{\\Gamma(\\alpha + \\beta)}{\\Gamma(\\alpha) \\Gamma(\\beta)} x^{\\alpha - 1} (1-x)^{\\beta-1} \\text{ for } x \\in [0,1] \\] Expected value: \\[ E(X) = \\frac{\\alpha}{\\alpha + \\beta} \\] Variance: \\[ V(X) = \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta+1)} \\] There are some properties that are important to know. If \\(G_{\\alpha} \\sim Gamma(\\alpha,1)\\), \\(G_{\\beta} \\sim Gamma(\\beta,1)\\), and the two are independent, \\[ \\frac{G_{\\alpha}}{G_{\\alpha} + G_{\\beta}} \\sim Beta(\\alpha, \\beta) \\] If \\(B \\sim Beta(\\alpha, \\beta)\\), then \\(1 - B \\sim Beta(\\beta, \\alpha)\\) \\(Beta(1,1) \\sim Unif(0,1)\\) "],["transformations-of-a-random-variable.html", "2 Transformations of a Random Variable 2.1 Monotonic Transformations 2.2 Non-monotonic Transformations", " 2 Transformations of a Random Variable Transformations of a Random Variable involve finding a PDF from a function form \\(Y = g(X)\\). If we know the PDF of \\(X\\) (\\(f_X(x)\\)), we can solve for the PDF of \\(Y\\) (\\(f_Y(y)\\)). 2.1 Monotonic Transformations Monotonic means that a function is strictly increasing or strictly decreasing without a change in the sign of the derivative. Suppose that \\(X \\sim f_X(s)\\) and \\(Y = g(X)\\). Assume \\(g(\\cdot)\\) is monotonic on the support of \\(X\\). Assume that the inverse function has a derivative that exists: \\[ \\frac{dX}{dy} = \\frac{dg^{-1}(y)}{dy} \\] The PDF of \\(Y\\) is given by: \\[ f_Y(y) = f_X(g^{-1}(y)) \\left| \\frac{dg^{-1}(y)}{dy} \\right| \\] A trick to remember this is: \\[\\begin{align*} f_Y(y) dy &amp;= f_X(x) dx\\\\ f_Y(y) &amp;= f_X(x) \\frac{dx}{dy}\\\\ &amp;\\text{Because } x = g^{-1}(y) \\dots\\\\ f_Y(y) &amp;= f_X(g^{-1}(y)) \\left| \\frac{d (g^{-1}(y))}{dy} \\right| \\end{align*}\\] The steps to solve a monotonic transformation problem are as follows: Check if the transformation is monotonic Transform the support (the domains, as in turning \\(x\\) into \\(y\\)) Find the inverse function \\(g^{-1} (y)\\) Substitute this inverse function into the transformation theorem and solve for \\(f_Y (y)\\) 2.1.1 Transformations of Location Scale Families If \\(X\\) is a location scale distribution \\(X \\sim LS(\\Theta_1, \\Theta_2)\\) and \\(Y = g(X)\\) where \\(g\\) is a linear function (as in \\(g(X) = aX + b\\)), then \\(Y \\sim LS(a \\Theta_1 + b, a^2 \\Theta_2)\\). 2.2 Non-monotonic Transformations Steps to solve are as follows. This method divides the transformation into monotonic subsets on which the transformation can be performed: Check that the transformation is non-monotonic Find the inverse function \\(g^{-1}(y)\\) Transform the support Define the transformation in terms of the CDF Divide the resulting inequality into pieces Differentiate to obtain the PDF and solve Transforming the support in this case requires the transformation of the endpoints (\\(x \\in [x_1, x_2]\\)) and the extreme values of \\(g(X)\\) (where \\(\\frac{dg(x)}{dx} = 0\\)). The support for \\(Y\\) is the minimum and maximum (\\(y \\in [\\min(g(x)), \\max(g(x))]\\)). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
