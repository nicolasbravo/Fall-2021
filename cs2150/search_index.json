[["index.html", "Midterm 2 Lecture Notes CS 2150 1 Trees 1.1 Binary Search Trees 1.2 Expression Trees 1.3 AVL Trees 1.4 Recursion 1.5 Red-black trees 1.6 Splay Trees 1.7 Tree applications", " Midterm 2 Lecture Notes CS 2150 Nico Bravo 1 Trees 1.1 Binary Search Trees 1.1.1 Relationship between \\(H\\) and \\(N\\) Given \\(n\\) nodes and height \\(h\\), then by the claim \\(n \\leq 2^{h+1}=1\\) We can simplify: \\(n+1 \\leq 2^{h+1}\\) \\(\\log_2(n+1) \\leq \\log_2(2^{h+1})\\) \\(\\log_2(n+1) \\leq h+1\\) Thus, \\(h \\geq \\log_2(n+1)-1\\) This means that the shortest tree we can achieve for \\(n\\) nodes is proportional to the base-2 log of the height 1.1.2 Perfect binary tree All leaves have the same depth And all nodes have 0 or 2 children (not 1) Number of leaves: \\(2^h\\) Number of nodes: \\(1 + 2 + 2^2 + 2^3+ \\dots + 2^h = 2^{h+1}-1\\) Problem: a perfect binary only holds \\(2^{h+1}-1\\) values So you can’t have 5 values in a perfect binary tree 1.2 Expression Trees A way to keep an internal representation of a mathematical equation Allows a computer to compute the value of each node 1.2.1 Expression tree traversals Infix notation: (a + ((b + c) * d)) Postfix notation: a b c + d * + Prefix notation: + a * + b c d 1.2.2 Building an expression tree Read tokens from input If a number or variable, push onto stack If an operator, pop off two values, attach them as children of the operator, and push back onto stack Proper postfix expression only has one value in stack once complete Example Consider the postfix expression a b + c d e + * 1. Read a, push onto the stack 2. Read b, push onto the stack 3. Read +, pop off a and b, put them as children of +, and push back onto stack 4. Read c, push onto the stack 5. Read d, push onto the stack 6. Read e, push onto the stack 7. Read +, pop off d and e, put them as children of +, and push back onto stack 8. Read , pop off + and c, put them as children of , and push back onto stack 9. Read , pop off * and +, put them as children of *, and push back onto stack This is the final result, as there is only one value left in the stack. 1.3 AVL Trees 1.3.1 Basics Motivation: to guarantee \\(\\Theta(\\log n)\\) running time on find, insert, and remove Idea: keep tree balanced after each operation Solution: AVL trees 1.3.2 Structure Property For every node in the tree, the height of the left and right sub-trees differs at most by 1 Height = root to most distant leaf If we keep inserting things in left or right subtree, they must be rebalanced Essentially is a binary search tree, but it gets rebalanced if the heights differ by at most 1 (1 difference is ok). 1.3.3 Balance Factor Each node of a BST holds: The data Left and right child pointers Possibly a parent node pointer An AVL tree also holds a balance factor The height of the right subtree minus the height of the left subtree We could have it be left minus right, but the convention in this class is to always have it be right minus left Can be computed on the fly, as well, but that’s VERY slow, and defeats the purpose of using AVL trees for speed 1.3.4 Tree Balance “Balanced” trees 0 means balanced 1 means the right subtree is one longer than the left subtree -1 means the left subtree is one longer than the right subtree “Unbalanced” trees A balance factor of -2 or 2 We’ll fix the tree Will we ever hit -3 or 3? No, because we immediately fix it after each operation if it reaches -2 or 2 By definition, a BST is a valid AVL tree if the balance factor for EVERY node is -1, 0, or 1 The balance factor is tracked for every node individually, so some can be balanced while others can be unbalanced 1.3.5 Find, insert find: same as BST find insert: same as BST insert, except might need to “fix” the AVL tree after the insert (via rotations) Runtime analysis: \\(\\Theta(d)\\), where \\(d\\) is the depth of the node being found/inserted What is the maximum ehight of an \\(n\\)-node AVL tree? 1.3.6 Tree operations Perform the operation (insert, delete) Move back up to the root, updating the balance factors Why only those nodes? Because those are the only ones who have had their subtrees altered Do tree rotations where the balance factors are 2 or -2 1.3.7 How many times to “fix” the tree? Any single insert will only modify the balance factor by one So we fix the lowest off-balance nodes Then everything above it is then balanced This means that we will have to only look at the bottom two unbalanced nodes 1.3.8 AVL Insert Let \\(x\\) be the deepest node where inbalance occurs Four cases where the insert happened: In the left subtree of the left child of \\(x\\) In the right subtree of the left child of \\(x\\) In the left subtree of the right child of \\(x\\) In the right subtree of the right child of \\(x\\) Cases 1 and 4 Perform a single rotation Cases 2 and 3 Perform a double rotation 1.3.9 AVL single right rotation On a tree with root 3 (-2), left child 2 (-1), left child to that 1 (0): The node just inserted was node 1 The lowest node, immediately after the insert, with an inbalance is node 3 Because node 1 is in the “left subtree of the left child” of node 3, this means we need to perform a single right rotation We end with a tree with 2 as root (0), with left child 1 (0) and right child 3 (0) This was “rotated” once to the right Left got too heavy (number was negative), rotate to right to balance 1.3.10 AVL single left rotation Mirror image of the single right rotation On a tree with root 1 (+2), right child 2 (+1), right child to that 3 (0): The node just inserted was node 3 The lowest node, immediately after the insert, with inbalance is node 1 Because node 3 is in the “right subtree of the right child” of node 1, this means we need to perform a single left rotation We end with a tree with 2 as root (0), with left child 1 (0) and right child 3 (0) This was “rotated” once to the left Right was too heavy (number was positive), rotate to left to balance 1.3.11 Side-effect of tree rotations At least one node moves “up” (depth decreases) At least one node moves “down” (depth increases) 1.3.12 Double rotation Two single rotations, in different directions! 1.3.13 Runtime Analysis Find \\(\\Theta(\\log n)\\) time Height of tree is always \\(\\Theta(\\log n)\\) Insert \\(\\Theta(\\log n)\\) time find() takes \\(\\Theta(\\log n)\\), then may have to visit every node on the path back up to root to perform up to 2 single rotations Remove \\(\\Theta(\\log n)\\) time Left as an exercise Print \\(\\Theta(n)\\) time No matter the data structure, it will still take \\(n\\) steps to print \\(n\\) elements 1.4 Recursion 1.4.1 Sum the numbers from 1 to \\(n\\) We can do this iteratively int sum (int n) { int s = 0; for ( int i = 1; i &lt;= n; i++ ) s += i; return s; } OR we can do this recursively What problems arise? int sum (int n) { return n + sum (n-1); } Recursion always needs three things to work A way to make the problem simpler or smaller A way to detect when it should terminate or end A way to terminate or end (the last example didn’t end) Now, let’s do the last example correctly unsigned int sum (unsigned int n) { if (n == 0) return 0; else return n + sum (n-1); } Negative numbers break this, which is why unsigned was added 1.4.2 Factorial via recursion unsigned long fact (unsigned long n) { if (n == 0) return 1; else return n * fact (n-1); } 1.4.3 Pros and cons Pros It’s a more natural way to think about the problem, as you only focus on one “instance” Some problems work well with recursion, but not with iteration (like tree traversals) Cons Invokng a subroutine at each step slows performance 1.4.4 Fibonacci Sequence unsigned int fib (unsigned int n) { if ( n &lt;= 1 ) return 1; else return fib(n-1) + fib(n-2); } This is incredibly inefficient, it is much quicker to do this in a while loop 1.4.5 Recursion on trees void func (BinaryNode *node) { if ( node == NULL ) return; func (node-&gt;left); do_something_with_node_value (node); func (node-&gt;right); } You cannot traverse a tree via iteration (not easily at at least…) 1.4.6 Recursion on lists bool is_in_list (int x, ListNode *l) { if ( l == NULL ) return false; else if ( x == l-&gt;value ) return true; else return is_in_list (x, l-&gt;next); } It would probably be better with iteration, but this just shows recursion is possible 1.4.7 Tail recursion The non-tail recursive version of sum(): unsigned int sum (unsigned int n) { if ( n == 0 ) return 0; else return n + sum (n-1); } Note that after the recursive call is completed, then we have to do something else We need to keep track of n, and then we need to add them together and return that value This is a performance penalty Compare this to the tail recursive version of sum(): unsigned int sum (unsigned int n, unsigned int s) { if ( n == 0 ) return s; else return sum (n-1, n+s); } The recursive call is the only thing returned by this call Whatever return returns is the only thing we worry about We don’t have to do anything else to it The recursive call can replace the return of the one-level-up call 1.4.8 Why tail recursion? Compilers recognize tail recursion and turn it into a for loop And then optimize, so there is no subroutine invocation penalty We get to write as recursion with speed benefits of iteration Not all problems can be solved with tail recursion, but for example the list recursion example was tail recursive 1.4.9 Factorial via Tail Recursion unsigned long fact_tr (unsigned int n, unsigned long s) { if ( n == 0 ) return s; else return fact_tr (n-1, n*s); } unsigned long fact (unsigned int n) { return fact_tr (n,1); } 1.5 Red-black trees Always used instead of AVL trees Each node has a color attribute, with is either red or black Saved as a bool, probably 1.5.1 Properties A node is either red or black The root is black All leaves are black The leaves may be the NULL children Both children of every red node are black Therefore, a black node is the only possible parent for a red node Every simple path from a node to any descendant leaf contains the same number of black nodes Counting or not counting the NULL black nodes, it doesn’t make a difference as long as you are consistent 1.5.2 Insert Insert the node as for a normal BST Color it red 5 possible cases: The new node is the root node Thus, must be painted black, as per rule 2 This adds a black node to every path in the tree, so property 5 still holds The new node’s parent is black Property 4 is not invalidated Property 5 is not threatened The new node’s children are black, but that’s the same number of black nodes as the node it replaced Both the parent and uncle are red We change grandparent to red Parent and uncle change to black Property 5 still holds However, grandparent may threated property 2 or 4 Recursively do this on the grandparent Occurs prior to any rotations Parent is red, uncle is black, new node is the right child of parent Perform a left rotation on P and N Property 4 is still violated Now treat it as case 5 Parent is red, uncle is black, new node is the left child of parent Perform a right rotation on P and G This satisfies properties 4 and 5 1.5.3 Removal Do a normal BST remove Find next highest/lowest value, put its value in the node to be deleted, remove that highest/lowest node That node won’t have 2 children We replace the node to be deleted with left child This child is N, sibling is S, parent is P 6 possible cases N is the new root S is red P, S, and S’s children are black S and S’s children are black, but P is red S is black, S’s left child is red, S’s right child is black, and N is the left child of its parent S is black, S’s right child is red, and N is the left child of parent P 1.5.4 Why red-black vs AVL? AVL trees are more rigidly balanced than red-black trees More rotations required Time-critical applications have a performance boost Functional programming languages use red-black trees for hashes/associative arrays 1.6 Splay Trees A self-balancing tree that keeps “recently” used nodes close to the top This improves performance in some cases Great for cashes Not good for uniform access Easier to implement than red-black or AVL trees 1.6.1 Main operation: splaying Anytime you find/insert/delete a node, you splay the tree around that node Perform tree rotations to make that node the new root node Binary search tree find Then, rotate that node to the top of the tree Then, just look at the root Animation site This is good, as the most recently looked up node is at the root, and nodes that have been recently looked for are near the root If a value is not found, the tree rotates around where the note would be If inserting a node, find where it would be, insert, then splay to make the inserted node the root 1.6.2 Time complexity How long does a splay take? \\(\\Theta(h)\\), where \\(h\\) is the height of the tree A splay tree can look like a linked list Which means the worst case running time can be linear, and the running time can be \\(\\Theta(n)\\) A splay tree is \\(\\Theta(\\log n)\\) amortized time This means a sequence of operations will average to \\(\\Theta(\\log n)\\) time 1.6.3 Amortized analysis An AVL tree will take \\(\\Theta(\\log n)\\) time for each and every operation in a series of \\(m\\) operations, guaranteed A splay tree will take an average of \\(\\Theta(\\log n)\\) time for the same set of \\(m\\) operations Some individual ones will take less time The total cumulative time to insert a set of elements will be the same for both types of trees Consider a vector insert Worst case is that you have to double the size of the array (from \\(n\\) to \\(2n\\)), and then copy the elements over, which takes \\(n\\) steps This doesn’t happen often though Consider a vector of initial capacity 100 elements, in which we insert 1,600 elements \\(n\\) = 1600 First “doubling” is from 100 \\(\\rightarrow\\) 200, costing 100 copies Second is 200 \\(\\rightarrow\\) 400, costing 200 copies Third is 400 \\(\\rightarrow\\) 800, costing 400 copies Fourth is 800 \\(\\rightarrow\\) 1600, costing 800 copies Assume we have not (yet) doubled from 1600 \\(\\rightarrow\\) 3200 For 1,600 elements, we have 100+200+400+800=1500 copies \\(n\\) elements requires \\(n-100\\) copies Number of copies is proportional to \\(n\\) Vector insert is \\(\\Theta(n)\\) for the worst-case \\(\\Theta(1)\\) amortized when executing many operations Although an individual operation may take \\(n\\) steps, a series of \\(c\\) operations will take \\(c \\cdot n\\) steps Where \\(c\\) is constant And when you divide by \\(n\\), then each operation takes an average of \\(c\\) steps 1.6.4 Splay tree conclusions Pros Easy to implement \\(\\Theta(\\log n)\\) amortized time Good for caches where locality matters Cons Not guaranteed \\(\\Theta(\\log n)\\) time for a single operation Bad access time for uniform access 1.7 Tree applications 1.7.1 When are trees not good to use? They’re fast, so when would we NOT want to use them? When the items don’t have a sorted order Ex. a list of to-do items When we want less complexity Ex. a stack or queue When we want \\(\\Theta(1)\\) operation on retrieves Vector get() When we want \\(\\Theta(1)\\) time for all operations Hash tables almost achieve that 1.7.2 Programs Any program can be represented as a tree Consider the following program: int z; int foo (int x) { for ( int y = 0; y &lt; x; y++ ) cout &lt;&lt; y &lt;&lt; endl; } int main() { int z = 5; cout &lt;&lt; &quot;enter x&quot; &lt;&lt; endl; cin &gt;&gt; z; foo(z); } Note that there are two int z declarations One in main One global As a tree, it looks like this: The Program Tree This is called an “abstract syntax tree” or “parse tree” Each note can be a different type Having different propreties and different number of children A for loop node has four children A function node has at least three children A body node has a variable number of children Done through inheritance A compiler will build this tree in memory And trasverse it many times For example, to figure out which z to use Or for code generation Or for type checking Or for code optimization 1.7.3 Comparing two programs If we read in two programs and built parse trees for each, then compared their structure, We would be able to compare the programs while ignoring Function/method order Variable renaming Different comments 1.7.4 Measure of Structural Similarity “MOSS” A system for detecting software plagiarism Loads up all the programs for a class Do all \\(n^2\\) comparisons Display the most similar programs "],["hash-tables.html", "2 Hash Tables 2.1 Review 2.2 Hash Tables 2.3 Hash Function Summary 2.4 Separate Chaining 2.5 Open Addressing 2.6 Miscellaneous", " 2 Hash Tables 2.1 Review 2.1.1 Lists Operations find insert remove findKth Implementations: generally linear-time Array (vector) find: \\(\\Theta(n)\\) insert: \\(\\Theta(n)\\) worst case, often \\(\\Theta(1)\\) remove: \\(\\Theta(n)\\) findKth: \\(\\Theta(1)\\) Linked list find: \\(\\Theta(n)\\) insert: \\(\\Theta(1)\\) remove: \\(\\Theta(n)\\) findKth: \\(\\Theta(n)\\) 2.1.2 Stacks List with data handled last-in first-out Operations push pop top Implementations: generally constant-time Array (vector) push: \\(\\Theta(n)\\) worst case, often \\(\\Theta(1)\\) pop: \\(\\Theta(1)\\) top: \\(\\Theta(1)\\) Linked list push: \\(\\Theta(1)\\) pop: \\(\\Theta(1)\\) top: \\(\\Theta(1)\\) 2.1.3 Queues First-in first-out list Operations enqueue dequeue Implementations Array (vector) enqueue: \\(\\Theta(n)\\) worst case, often \\(\\Theta(1)\\) dequeue: \\(\\Theta(1)\\) Linked list enqueue: \\(\\Theta(1)\\) dequeue: \\(\\Theta(1)\\) 2.1.4 Trees Goal is \\(\\Theta(\\log n)\\) runtime for most operations (balanced trees are generally logarithmic-time operations) Binary search trees find: \\(\\Theta(h)\\), where \\(\\log n &lt; h \\leq n-1\\); worst case is \\(\\Theta(n)\\) insert: \\(\\Theta(h)\\), where \\(\\log n &lt; h \\leq n-1\\); worst case is \\(\\Theta(n)\\) remove: \\(\\Theta(h)\\), where \\(\\log n &lt; h \\leq n-1\\); worst case is \\(\\Theta(n)\\) AVL Trees find: \\(\\Theta(\\log n)\\) insert: \\(\\Theta(\\log n)\\) remove: \\(\\Theta(\\log n)\\) Red-black trees find: \\(\\Theta(\\log n)\\) insert: \\(\\Theta(\\log n)\\) remove: \\(\\Theta(\\log n)\\) Splay trees find: \\(\\Theta(\\log n)\\) amortized insert: \\(\\Theta(\\log n)\\) amortized remove: \\(\\Theta(\\log n)\\) amortized 2.2 Hash Tables 2.2.1 Is there anything faster than what was reviewed? Binary comparison searches are \\(\\Omega(\\log n)\\) We can do better: almost constant \\(\\Theta(1)\\) is possible with hash tables 2.2.2 Hash tables (lookup table) Fixed size array of some size, usually a prime number Should be larger than the number of elements Has a hash function Has a key, does manipulation on it to find where it goes on the table Standard set of operations find insert delete No ordering property (no findMin or findMax) 2.2.3 Key-value pairs Hash tables store key-value pairs Each value has a specific key associated with it Keys and values need not be the same type Example Dictionary: word and definition 2.2.4 Hash function Input/output Takes in a string, int, object, etc Outputs an unsigned integer value This is then mod’ed by the size of the hash table to yield a spot within the bounds of the hash table array Three required properties Must be deterministic Meaning must return/output the same value each time for the same input Must be fast Must be evenly distributed Technically, only the first is required for correctness, but the other two are required for fast running times 2.2.5 Keys How can we hash the keys if the keys can be anything? Best one binary comparison can do is eliminate one half of the elements \\(\\Theta(\\log n)\\) We want \\(\\Theta(1)\\) The keys must be bits, so we can do better Lookup table This can work, unless the key space is sparse, or we don’t know the keys ahead of time It’s slow to look up a value in a table Example Key space: integers Table size: 10 hash(k) = k%10 Technically, hash(k) = k, which is then mod’ed by the table size of 10 Insert: 7, 18, 41, 34 How do we find them? By looking up the hash value 2.2.6 Table size issues Why not just have a table of size 100 and map them directly to the location corresponding to their key? We assume the key space is too large (ex. all students’ SSNs… there are not 999,999,999 students at UVA) Do you see why findMax and findMin are not easy? We have not preserved any ordering information Another example Key space: integers Table size: 6 hash(k) = k%6 Insert: 7, 18, 41, 34, 12 How do we find them? A good hash function is required 2.2.7 Hash table part 2 Hash function: hash: key $\\rightarrow$ [0,$m$-1] Really to any unsigned integer, which is then mod’ed by \\(m\\), the table size Here, `hash(key) = firstletter(key) 2.2.8 Hash functions part 2 Required properties described earlier Must be deterministic Must be fast Must be evenly distributed This implies avoiding of collisions A perfect hash function has: No blanks (no empty cells) No collisions 2.2.9 Sample string hash functions Key space: strings A string \\(s\\) is made up of characters \\(s_i\\) \\(s = s_0 s_1 s_2 s_3 \\dots s_{k-1}\\) We can come up with different options for functions: hash(\\(s\\)) = \\(s_0\\) % table_size fast, deterministic, but not evenly distributed hash(\\(s\\)) = \\((\\sum_{i=0}^{k-1} s_i)\\) % table_size We can sum the characters instead hash(\\(s\\)) = \\((\\sum_{i=0}^{k-1} s_i \\cdot 37^i)\\) % table_size Take each letter, and multiply it by \\(37^i\\) 37 is for no reason other than it being a prime number Don’t use pow() function as it is slow (waste of computation), precompute the values instead As the word gets bigger, the size will increase It will overflow after a couple characters This doesn’t matter, as it is an unsigned int If we overflow the same way each time, we can still find it 2.3 Hash Function Summary Should always return an unsigned int Otherwise program will be trying to find a negative array index Integer overflow is fine, as long as it overflows deterministically Meaning same way every time Will especially be true with the last of the string hash functions mentioned (the third one) 2.3.1 Collision Resolution Collision: when two keys map to the same location in the hash table Two primary ways to resolve collisions Separate Chaining (make each spot in the table a ‘bucket’ or collection) Open addressing, of which there are three types: Linear probing Quadratic probing Double hashing 2.4 Separate Chaining All keys that map to the same hash value are kept in a “bucket” This “bucket” is another data structure, typically a linked list The worst case is linear time This is when everything maps to the same position in the table, then you get the last element Example: hash(k) = k%10 Insert: 10, 22, 107, 12, 42 2.4.1 Analysis of find Definition: the load factor \\(\\lambda\\) of a hash table is the ratio of the number of elements divided by the table size For separate chaining, \\(\\lambda\\) is the average number of elements in the bucket Average time on unsuccesful find: \\(\\lambda\\) Average length of a list at hash(k) Average time on successful find: \\(1 + (\\lambda/2)\\) One node, plus half the average length of a list (not including the item) 2.4.2 Load factor How big should we make the hash table? We want “constant” time for find and insert Possible sizes for hash table with separate chaining \\(\\lambda = 1\\) Make hash table be the number of elements expected Average bucket size is 1 Also make it a prime number \\(\\lambda = 0.75\\) Java’s hashtable but can be set to another value Table will always be bigger than number of elements Reduces the chance of a collision Good trade-off between memory use and time \\(\\lambda = 0.5\\) Uses more memory, but fewer collisions So… High load factor = less memory used, more collisions Lower load factor = more memory used, less collisions 2.4.3 Separate Chaining: find() Note that we now have to keep each key in the chain, as well as the value In the worst case, every key could hash to the same spot As nobody uses anything other than a linked list as the secondary data structure, a find will be \\(\\Theta(n)\\) 2.4.4 What data structure to use for the buckets? AVL &amp; red-black trees will give the best running time, but that’s a lot of overhead Vectors are easier and simpler, but take up a lot of space A lot of extra, unused cells Don’t ever use vectors for this! Linked lists are quick and easy, and take up very little extra space That’s \\(\\Theta(n)\\) Still faster in practice than trees due to having a very small number of items in the bucket For our analyses, we assume linked list as the bucket object Everyone uses linked lists for this anyways apparently 2.4.5 Requirements for “hopeful” case Our ideal hash function and hash table: Function hash(k) is well distributed for key space For a randomly selected \\(k \\in K\\), the probability that hash(\\(k\\)) = \\(i\\) is 1/table_size Size of table scales linearly with number of elements Expected bucket size is \\(\\Theta\\)(num_elements/table_size) Finding a good hash function can be hard 2.4.6 Sepearete Chaining insert is \\(\\Theta(1)\\) In an unsorted linked list, just put it in the front All inserts into a seperate chained hash table that uses linked lists are therefore constant time If it were sorted, then linear time Finds and deletes are still linear time 2.5 Open Addressing 2.5.1 Saving memory Can we avoid the overhead of all the linked lists used? 2.5.2 Three types of probing strategies The three types: Linear Quadratic Double hashing The general idea with all of them is that, if a spot is occupied, to ‘probe’, or try, other spots in the table to use How we determine where else to probe depends on which strategy we are using 2.5.3 Linear probing With all open addressing schemes, we examine (probe) the cells in the order: \\(p_0 (k), p_1 (k), p_2 (k), \\dots\\) Where \\(p_i (k) =\\) (hash(\\(k\\)) + \\(f(i)\\)) % table_size With linear probing, \\(f(i) = i\\) After searching spot hash(\\(k\\)) in the array, look in hash(k) + 1 hash(k) + 2 hash(k) + 3 etc, etc 2.5.4 Linear probing table example Chceck spots in order hash(k) hash(k) + 1 hash(k) + 2 etc, etc hash(k) = 3k+7 Which is then mod’ed by the table size (10 in this example) Result: hash(k) = (3k+7) % 10 Insert: 4, 27, 37, 14, 21 hash(k) values: 19, 88, 118, 49, 70 respectively What if we want to insert 1? \\(3(1)+7 = 10 \\% 10 = 0\\) 0 is used (37), keep moving up… 1 is used (14), move up 2 is used (21), move up cell 3 is empty! fill it with 1. Worst case is linear, but expected case is constant time Delete: What if we delete 14 (slot 1)? Then we find 21… \\(3(21) + 7 = 70 \\% 10 = 0\\) 0 is used (37), keep moving up… 1 is empty! 21 is not here BUT… 21 is actually in slot 2 This can be an issue 2.5.5 Problems with linear probing Primary clustering Large blocks of occupied cells As table fills, increased number of attempts required to solve collisions And slower lookup times “Holes” when an element is removed When to stop looking??? 2.5.6 Quadratic Probing With all open addressing schemes, we examine (probe) the cells in the order: \\(p_0 (k), p_1 (k), p_2 (k), \\dots\\) Where \\(p_i (k) =\\) (hash(\\(k\\)) + \\(f(i)\\)) % table_size With linear probing, \\(f(i) = i^2\\) After searching spot hash(\\(k\\)) in the array, look in hash(k) + 1 hash(k) + 4 hash(k) + 9 etc, etc 2.5.7 Quadratic probing table example Check spots in this order: hash(k) hash(k) + \\(1^2\\) = hash(k) + 1 hash(k) + \\(2^2\\) = hash(k) + 4 hash(k) + \\(3^2\\) = hash(k) + 9 etc, etc hash(k) = 3k+7 Which is then mod’ed by the table size (in this case 10) Result: hash(k) = (3k_7) % 10 Insert: 4, 27, 14, 37, 22, 34 hash(k) values: 19, 88, 49, 118, 73, 109 What is the benefit to this? Linear probing creates a cluster (as previously shown) where it has to iterate through cells Quadradic probing decreases the number of cells in a cluster 2.5.8 Double Hashing With all open addressing schemes, we examine (probe) the cells in the order: \\(p_0 (k), p_1 (k), p_2 (k), \\dots\\) Where \\(p_i (k) =\\) (hash(\\(k\\)) + \\(f(i)\\)) % table_size With linear probing, \\(f(i) = i *\\)hash\\(_2(k)\\) Which means we have to define a secondary hash function After searching spot hash(\\(k\\)) in the array, look in hash(k) + 1 * hash\\(_2(k)\\) hash(k) + 2 * hash\\(_2(k)\\) hash(k) + 3 * hash\\(_2(k)\\) etc, etc 2.5.9 Double hasing table example Check spots in this order: hash(k) hash(k) + 1 * hash\\(_2(k)\\) hash(k) + 2 * hash\\(_2(k)\\) hash(k) + 3 * hash\\(_2(k)\\) etc, etc hash(k) = k The hash function was made simpler for this example… Which is then mod’ed by the table size (10) Result: hash(k) = k % 10 hash\\(_2(k) = 7 - (k\\%7)\\) Insert: 89, 18, 58, 49, 69, 60 The advantage to this is that two things that hash to the same spot, the secondary hash value is going to likely be different The disadvantage is that you now need two hash functions :( 2.5.10 Double hashing thrashing Half of our table is available, but let’s say we add 36 to the example from the previous lecture hash(k) = k mod 10 36 % 10 = 6 hash2(k)=(k mod 5) + 1 36 % 5 = 1 + 1 = 2 If we keep adding two, we only have even spaces in the table! This means that half our table is going to be full and half is going to be empty, yet this will never be able to place into a space. Because of this… 2.5.11 Table size must be prime! The table size must always be a prime number It will prevent the thrashing Thrashing will only occur when the double hash value is a factor of the table size The only factors of a prime number \\(p\\) are 1 and \\(p\\) 1 is effectively linear probing, which is fine \\(p\\) will mod to zero, which is an invalid return value for a secondary hash function It will provide better distribution of the hash keys into the table Less clustering, etc. A prime number table size does not remove the need for a good hash function! 2.6 Miscellaneous 2.6.1 Rehashing Problem: when the table gets too full, running time increases Solution: create a bigger table (typically the next biggest prime) and hash all the items from the original table to the new table Position in a table is dependent on table size We need to rehash each value We need to re-compute the hash value for each element, take it out of the old table, and insert it into the new table This is \\(\\Theta (n^2)\\) running time Have \\(n\\) elements to rehash Have \\(n\\) elements to insert When to rehash? When half full (\\(\\lambda = 0.5\\)) When mostly full (\\(\\lambda = 0.75\\)) Java’s hashtable does this by default When an insertion fails Some other threshold 2.6.2 Removing an element How do you do this? There are a couple options… Rehash upon each delete Very expensive Put in a ‘placeholder’ or ‘sentinel’ value How do these work? It’s a character that we mark as the sentinel value If a delete happens, we replace the table value with the sentinel value Insert does not skip these If a new value matches to sentinel value, replace sentinel value Find does skip these Value to be found might be after sentinel value, so continue beyond sentinel value to find requested value The table gets filled with these rather fast Maybe rehash after a certain number of deletes and delete sentinel values? Don’t allow deletes Hash tables are not ideal for situations where a lot of deletes happen 2.6.3 Hashing: MD5 MD5: Message Digest 5 Given a string (or file contents, etc.) generate a 128-bit hash \\(2^{128} = 3.4 \\times 10^{38}\\) (coincidentally, this is also the maximum finite value of a float) Typically an MD5 is always written in hex: 16e28b7986fd74f65b061de89dc8b78e This could then be used as the key, after modding it by the table size (Was) good for checking if a download completed successfully 2.6.4 Can you reverse an MD5 hash? Technically, no A 129-bit file has \\(2^{129}\\) possibilities, and if you were to hash each one, it would go into \\(2^{128}\\) buckets By the pigeonhole principle, there would be at least one hash value (pigeonhole) with multiple keys (pigeons), and you don’t know which one In reality, many (and eventually all) would have multiple keys But if a password is stored by it’s MD5 hash… … then there are enough online hash libraries that you can find at least one password that hashes to that value Try Googling for 3858f62230ac3c915f300c664312c63f Plus there are lots of weaknesses in MD5… 2.6.5 More hashing: SHA MD5 has been “broken” One can generate two files that have the same hash; this is called a collision attack So it is useless for any security-related purposes SHA (Secure Hash Algorithm) is a family of algorithms that (the more recent ones) are much more secure Same overall idea: it generates a hash value up to 512 bits SHA-1 has been broken also, but more recent SHAs are secure "],["ibcm-machine-language.html", "3 IBCM (Machine Language) 3.1 Introduction 3.2 IBCM Description 3.3 Writing IBCM Code 3.4 Conclusions", " 3 IBCM (Machine Language) 3.1 Introduction 3.1.1 Assembly Language Machine language Instructions represented as patterns of bits (0s and 1s) that can be understood and processed by a central processing unit (CPU) Program stored in main memoryf Assembly language Human-readable notation for the machine language used to control a specific computer architecture Assembler translates to bits 3.1.2 Why learn assembly language? Machine designers Compiler writers Programmers (especially for OSes) Assembly language programmers Most importantly: helps you understand how computers compute 3.1.3 Memory Hierarchy, part 1 Memory Hierarchy CPU registers 1 access per CPU cycle \\(3 \\times 10^9\\) accesses per second 1 Kb total storage Cache SDRAM: 10 nanoseconds \\(10^8\\) accesses per second Multiple levels possible Higher levels are bigger and slower 1 Mb total storage 3.1.4 Memory Hierarchy, Part 2 Main memory DRAM: 60 nanoseconds \\(2 \\times 10^7\\) accesses per second Limited by bus speeds 1 gigabyte total storage Disk HDD speeds: 5 milliseconds 200 accesses per second 1 terabyte total storage 3.1.5 Fetch Execute Cycle while(power is on) { IR := memory[PC] Increment PC by length of instruction execute instruction in IR } PC = program counter IR = instruction register 3.1.6 Assembly Language Instructions x86 Explicit use of registers add eax, ebx add ecx, 1 IBCM Implied use of accumulator load 100 add 200 store 300 3.2 IBCM Description 3.2.1 Running IBCM Programs Online IBCM simulator https://pegasus.cs.virginia.edu/ibcm/ https://libra.cs.virginia.edu/ibcm/ Write program as text file, load into website This program will hang your browser if it gets stuck in an infinite loop, due to how web browsers (don’t) handle Javascript threads and polling of web pages 3.2.2 IBCM Machine Description: CPU Single accumulator 16 bits Special purpose registers IR: instruction register Stores bits which encode instruction PC: program counter Stores an address of an instruction 3.2.3 IBCM Machine: Memory 4096 16-bit words Word “chunk size” or addressable unit All initialized to zero initially Unlike C/C++ All spots go from 0 to fff, as 4096 = \\(2^{12}\\) 3.2.4 IBCM instruction types Halt Opcode is 0 I/O Opcode is 1 next 2 bits specify I/O type Shifts Opcode is 2 next 2 bits specify shift type last 4 bits specify shift amount Others Opcode is 3 to F (15) Last 12 bits specify the address for the instruction The arithmetic, memory, and control instructions 3.2.5 Halt The opcode (bits 15 down to 12) is zero It doesn’t matter what the next 12 bits are It halts the IBCM! 3.2.6 Input and output The opcode (bits 15 down to 12) is 1 Next two bits specify I/O type: Bit 11 specifies input (0) or output (1) Bit 10 specifies hex word (0) or ascii (1) Combinations: 00: read hex word from keyboard 01: read ascii character (into acc bits 7-0) 10: write hex word to screen 11: write ascii character (from acc bits 7-0) 3.2.7 Shifts The opcode (bits 15 down to 12) is 2 Next two bits specify shift type: Bit 11 specifies shift (0) or rotate (1) Shift: add new digits, shift everything to a direction Rotate: wrap the digits around to the end Bit 10 specifies direction: left (0) or right (1) Consider the bits [0000] [1111] [0000] [1111] and a 3-bit shift/rotation (bold is new digits added): 00: shift left: 0][111 1][000 0][111 1]000 01: shift right: 000[0 000][1 111][0 000][1 10: rotate left: 0][111 1][000 0][111 1][000 11: rotate right: 111][0 000][1 111][0 000][1 3.2.8 Other Instructions Opcode (bits 15 down to 12) varies from 3 to F (15) Next 12 bits specifies the address 3.2.9 Labels We often need to jump around our code For this we use labels in assembly language These are translated into absolute memory addresses later by assembler We don’t have an assembler in IBCM, so we will have to translate them ourselves start readH loop load n xit load s 3.2.10 Declaring variables You use the dw opcode It stands for Declare Word, perhaps And you need to give it a label, so you can reference it in assembly language program Example: n dw 0 declares a spot with label ‘n’ to have value 0 load n will load the value stored in that spot in memory 3.2.11 Sample Program Address 000 Contents: 3000 Command: load mem[0] Load the number in memory 0: 3000 Address 001 Contents: 5000 Command: add mem[0] Add number in memory 0 to the number we are working with (accumulator) 3000 + 3000 = 6000 Address 002 Contents: 6001 Command: sub mem[1] Subtract accumulator with what is in memory 1 6000 - 5000 = 1000 Address 003 Contents: 8003 Command: or mem[3] OR accumulator and the contents of this memory address 0x1000 = 0001 0000 0000 0000 0x8003 = 1000 0000 0000 0011 OR these two values: 1001 0000 0000 0011 = 0x9003 Address 004 Contents: a000 Command: not Change all 1s to 0s, and all 0s to 1s 0110 1111 1111 1100 = 0x6ffc Address 005 Contents: 4000 Command: store mem[0] Write 0x6ffc into memory Address 006 Contents: f000 Command: brl mem[0] Store the address of the next instruction in the accumulator (007) and jump to address 000 3.3 Writing IBCM Code Write high level pseudo-code for ( i = 1; i &lt; max; i++ ) ... Translate into IBCM assembly instructions load one store i Test code by hand Step through the code Encode into machine code 3016 4005 Load machine code into simulator and run 3.3.1 Simulator Leave “load all of memory” unchecked as no programs in this class will be large enough to fill Buttons Run Run to finish Step Run next step Reset Go back to start, but do not modify any values in memory Revert Revert values to back when we first loaded file 3.3.2 IBCM code to compute a sum Compute the sum of integers 1 through \\(n\\), where \\(n\\) is to be read from the keyboard Resulting sum is to be printed to the screen Half after printing the sum Source code: summation.ibcm Pseudocode: read n; i = 1; // index in the array s = 0; // ongoing sum while (i &lt;= n) { s += i; i += 1; } print s; Given a number n, finding \\(\\sum_{i=1}^n x_i\\) 3.3.3 IBCM summation program: part 1 mem locn label op addr comments C00A 000 jmp start skip around the vars 0000 001 i dw 0 int i 0000 002 s dw 0 int s 0000 003 n dw 0 int n 0001 004 one dw 1 0000 005 zero dw 0 ... 1000 00A start readH read n 4003 00B store n 3004 00C load one i = 1 4001 00D store i 3005 00E load zero s = 0 4002 00F store s Note that the top line isn’t in the actual program: there cannot be any blank lines We have 5 variables i s n one zero No way to add a constant to a value, so we created the variables one and zero to be able to add dw = declare word C00A Unconditional jump C means jump Jump to location 00A 1000 readH = read hex (read n) 4003 Store what is in 3: n 3004 Load what is on location 4: one 4001 Store what is loaded (one) into i i = 1 Same thing for 3005 and 4002 Load value of zero Store zero into s 3.3.4 IBCM summation program: part 2 mem locn label op addr comments 3003 010 loop load n if (i &gt; n) goto xit 6001 011 sub i E01A 012 jmpl xit 3002 013 load s s += i 5001 014 add i 4002 015 store s 3001 016 load i i += 1 5004 017 add one 4001 018 store i C010 019 jmp loop goto loop 3002 01A xit load s print s 1800 01B printH 0000 01C halt halt We cannot compare i and n directly We can instead subtract i from n If negative, jump to xit jmpl Jump if less than 0 s is the sum Add i to s Add one to i jmp back into the loop line 3.3.5 How would we code this: if (B == 0) S1; else S2; load B jmpe S1 jmp S2 S1: ... (if) jmp done S2: ... (else) done 3.3.6 IBCM code to sum elements in an array Compute the sum of the elements of an array, print the sum, then halt Address of the first element of the array and the size of the array are to be read in from the keyboard Source code: array-summation.ibcm Pseudocode: read a; // array base address read n; // array size i = 0; // index in the array s = 0; // ongoing sum while (i &lt; n) { s += a[i]; i += 1; } print s; 3.3.7 Array Summation: initialization and halting mem locn label op addr comments C00A 000 jmp start skip around the vars 0000 001 i dw 0 int i 0000 002 s dw 0 int s 0000 003 a dw 0 int a[] 0000 004 n dw 0 0000 005 zero dw 0 0001 006 one dw 1 5000 007 adit dw 5000 ... leave space for changes 1000 00A start readH read array address 4003 00B store a 1000 00C readH read array size 4004 00D store n 3005 00E load zero i = 0; s = 0; 4001 00F store i 4002 010 store s ... 3002 020 xit load s print s 1800 021 printH 0000 022 halt 3.3.8 Array Summation: the inner loop mem locn label op addr comments ... 3004 011 loop load n if (i &gt;= N) goto xit 6001 012 sub i E020 013 jmpl xit D020 014 jmpe xit 3007 015 load adit form inst to add a[i] 5003 016 add a 5001 017 add i i = 2 401A 018 store doit plant inst into code 3002 019 load s s += a[i] 0000 01A doit dw 0 executes from locn 018 4002 01B store s 3001 01C load i i += 1 5006 01D add one 4001 01E store i C011 01F jmp loop goto loop ... jmpl = jump if less than jmpe = jump if equal to 3.4 Conclusions 3.4.1 Notes IBCM’s memory Array of “words” / chunks of data Data can be the program Program can be the data IBCM is Turing-complete Any program expressible in any programming language can be expressed in IBCM (with the standard exception about the whole limited-memory thing) 3.4.2 IBCM tips/reminders Use the programming steps Pseudocode Assembly code Comment your code clearly! Trace the assembly Translate to machine code last Cannot have blank lines or comment lines 3.4.3 Use the simulators to debug your code Online simulator Cannot terminate an infinite loop easily Unusual behavior? Is the program logic correct? Condition specified incorrectly/inaccurately? Is the machine code correct? Opcode? Address? 3.4.4 What’s missing from IBCM? Integer multiply and divide Floating point support A bigger address space More than 1 user register What else? 3.4.5 Emulating IBCM in C++ How might one write software to emulate an IBCM machine in C++? A switch statement with 16 cases, perhaps But how to decode the instructions? 3.4.6 How to decode the parts of an instruction Let’s assume we had to write a C++ program that could extract the parts of an IBCM instruction How to do it? Assume the instruction is in an unsigned int x unsigned int opcode = (x &gt;&gt; 12) &amp; 0x000f unsigned int ioshiftop = (x &gt;&gt; 10) &amp; 0x0003 unsigned int address = x &amp; 0x0fff unsigned int shiftcount = x &amp; 0x000f 3.4.7 What about encoding? Assume we have (unsigned ints) opcode, ioshiftop and shiftcount unsigned int instruction = (opcode &lt;&lt; 12) | (ioshiftop &lt;&lt; 10) | shiftcount What a pain this all is! 3.4.8 A data structure to make it easier union ibcm_instruction { #ifdef BIG_ENDIAN // the IBCM is big endian struct { unsigned char high, low; } bytes; #else #ifdef LITTLE_ENDIAN struct { unsigned char low, high; } bytes; #else #error Must define BIG_ENDIAN or LITTLE_ENDIAN #endif // LITTLE_ENDIAN #endif // BIG_ENDIAN struct { unsigned int op:4, unused:12; } halt; struct { unsigned int op:4, ioopt:2, unused:10 } io; struct { unsigned int op:4, shiftop: 2, unused:5, shiftcount:5; } shifts; struct { unsigned int op:4, address:12; } others; }; 3.4.9 Using that data structure // read in instruction into unsigned chars a and b ibcm_instruction inst; inst.high = a; inst.low = b; if ( inst.halt.op == 0 ) { // halt // ... } else if ( inst.io.op == 1 ) { // io cout &lt;&lt; inst.io.ioopt &lt;&lt; endl; } else if ( inst.shifts.op == 2 ) { // shifts cout &lt;&lt; inst.shifts.shiftop &lt;&lt; endl; cout &lt;&lt; inst.shifts.shiftcount &lt;&lt; endl; } else { // all others cout &lt;&lt; inst.others.address &lt;&lt; endl; } "],["bit-x86.html", "4 64 Bit x86 4.1 Introduction to x86 4.2 x86 Instruction Set 4.3 Calling Conventions 4.4 Caller Rules 4.5 Callee Rules 4.6 Activation Records 4.7 x86 Examples", " 4 64 Bit x86 4.1 Introduction to x86 4.1.1 Registers rax = 64 bits eax = right 32 bits ax = right 16 bits al = right 8 bits ah = next 8 bits 4.1.2 IBCM vs x86: fetch execute cycle (same) while(power is on) { IR := mem[PC] PC := PC + 1 (word) // 64-bits in x86 execute instruction in IR } PC = program counter IR = instruction register 4.1.3 Declaring variables in x86 Directives byte: 1 byte (DB) declare byte word: 2 bytes (DW) double: 4 bytes (DD) quadword: 8 bytes (DQ) TIMES x DB 0 directive means create x bytes of value zero section .data a DB 23 b DW ? c DD 3000 d DQ -800 x DD 1, 2, 3 y TIMES 8 DB 0 str DB &#39;hello&#39;, 0 z TIMES 50 DD ? When writing assembly, will write data section and text section 4.1.4 Mov command mov &lt;dest&gt;, &lt;src&gt; Where dest and src can be: A register A constant Variable name Pointer: [rbx] *You will often see movq (move quad word) or movl (move double word), etc. This is more precise as you are declaring how many bytes will be moved, but depends on the assembly syntax used (more on this later) 4.1.5 Addressing memory Up to 2 registers and one 64-bit signed constant can be added together to compute a memory address Furthermore, one register can be pre-multiplied by 2, 4, or 8 word-align double-align quadword-align Examples: mov rax, rbx mov rax, [rbx] mov [var], rbx mov rax, [r13 - 4] mov [rsi + rax], cl mov rdx, [rsi + 4*rbx] INCORRECT examples: mov rax, [r11 - rcx] mov [rax + r5 + rdi], rbx mov [4*rax + 2*rbx], rcx 4.1.6 Example mov rcx, rax mov rdx, [rbx] mov rsi, [rdx+rax+16] mov [rsi], 45 mov [a], 15 lea rdi, [a] mov rcx, rax Move value of rax into rcx For example, rax = 100 Now rcx = 100 mov rdx, [rbx] Move the value in the memory address of rbx (this is what the square brackets are for) and move it into rdx For example, rbx = 108 Memory address 108 is 8 rdx = 8 mov rsi, [rdx+rax+16] Move value in the memory address of rdx+rax+16 and move it into rsi rdx = 8, rax = 100, rdx+rax+16 = 124 Move value of 124 = 200 into rsi rsi = 200 mov [rsi], 45 Move the value 45 into the memory address that matches the value of rsi Value of rsi = 200 Move 45 into address 200 mov [a], 15 Move the value 15 into the memory address of the value of a For example, a is at memory address 300, so address 300 = 15 lea rdi, [a] Sets rdi to the memory address a rdi would equal 300 4.1.7 Valid or not? mov rax, [4*rsi-rdx] Not valid! Cannot subtract mov rax, [4*rsi+4] Yes mov rax, [4*rsi+rdx+8] Not valid Cannot do more than one multiplication and one addition because of clock speed mov rax, [rsi+4*rdx] Yes mov rax+8, [rsi] You can do this, but needs to be in square brackets No, this is not valid 4.1.8 Memory addressing restrictions The destination cannot be a constant (duh!) You cannot access memory twice in one instruction Instead, would need to make another variable temp As the CPU does not have enough time to do so at that clock speed So the following instructions are invalid: mov [rax], [var] mov [rax+8], [rbx] mov 20, [rax] Also, cannot put values into constants Destination cannot be a constant 4.2 x86 Instruction Set Data movement instructions Arithmetic instructions Logical instructions Control instructions 4.2.1 Data movement instructions mov (or movq if you are moving 8 bytes) We’ve seen this in detail already push First decrements register RSP (stack pointer) by 8 (stack grows down) push (mov) operand onto stack (8 bytes) sometimes you will see this as pushq (push quad word onto stack) push rax push [var] pop Pop top element of stack to memory or register, then increment stack pointer (RSP) by 8 Value is written to the parameter pop rax pop [var] lea Load effective address Place address of second parameter into the first parameter Every time you have square brackets around a register, it’s the memory address You can use this to do math (like the second example below) lea rax, [var] lea rdi, [rbx+4*rsi] 4.2.2 Arithmetic instructions add, sub Adds (or subtracts), storing result in first operand add &lt;reg&gt;, &lt;reg&gt; add &lt;reg&gt;, &lt;mem&gt; add &lt;mem&gt;, &lt;reg&gt; add &lt;reg&gt;, &lt;constant&gt; add &lt;mem&gt;, &lt;constant&gt; Similar restrictions as with data movement instructions: Destination cannot be a constant Memory cannot be accessed twice inc, dec (increment and decrement by one) inc &lt;reg&gt; inc &lt;mem&gt; //Specific examples dec rax inc [var] imul imul &lt;reg64&gt;, &lt;reg64&gt; (or &lt;mem&gt;) imul &lt;reg64&gt;, &lt;reg64&gt; (or &lt;mem&gt;), &lt;con&gt; idiv Divide 128-bit integer in RDX:RAX by operand idiv rbx 4.2.3 Logical instructions and, or, xor The second example is a quick way to zero out a register and &lt;reg&gt;, &lt;reg&gt; and &lt;reg&gt;, &lt;mem&gt; and &lt;mem&gt;, &lt;reg&gt; //Specific examples: and rax, 0fH xor rcx, rcx 4.2.4 Control Instructions jmp &lt;label&gt; go to instruction address specified by label cmp This must be done prior to a conditional jump cmp operand1, operand2 Operand1 can be a register or memory (variable) Operand2 can be a register, memory (variable), or a constant Recall that you can’t access memory twice! Sets the machine status word Conditional jumps: jcondition Uses the machine status word, which was set via cmp Which holds info about the results of the last instruction There are many ’condition’s to determine whether to jump Example: je &lt;label&gt; Jump when condition code equal is set Others: jne, jz, jg, jge, jl, jle, js, etc. call &lt;label&gt; Subroutine call Pushes address of the next instruction onto the stack, then unconditionally jumps to the label ret Subroutine return Pops the return address from the stack, then jumps to that address 4.2.5 A code block in both C/C++ and Assembly C/C++ code: int n = 5; int i = 1; int sum = 0; while (i &lt;= n) { sum += i; i++; } Assembly code: section .data n DQ 5 i DQ 1 sum DQ 0 section .text loop: mov rcx, [i] cmp rcx, [n] jg endOfLoop add [sum], rcx inc qword [i] jmp loop endOfLoop: 4.3 Calling Conventions 4.3.1 Calling of a subroutine: C++ int max(int x, int y) { int theMax = (x &gt; y) ? x : y; return theMax; } int main() { int a = 5, b = 6; int maxVal = max(a,b); cout &lt;&lt; &quot;Max value: &quot; &lt;&lt; maxVal &lt;&lt; endl; return 0; } 4.3.2 Calling Conventions What is a calling convention? A set of rules/expectations between functions How (and where) are parameters passed? Which registers does the calling function expect to be preserved? Where should local variables be stored? How/where should results be returned from functions? Why? Separate programmers can: Share code more easily Develop libraries 4.3.3 C Calling Convention Why C’s calling convention? It’s important Is used with both C and C++ code Can enable calling C library functions from assembly code Or other languages, too Uses hardware stack (memory) Stack grows down, toward the lower memory addresses x86 instructions used for calling convention pop push call ret Using a stack for calling convention is implemented on most processors. Why? Recursion 4.3.4 C Calling Convention Overview Answers to questions Parameters: passed in registers If more than 6, then params \\(7-n\\) placed on stack If passing a large object, then placed on stack (doesn’t fit in 64-bit register) Registers: saved on the stack Local variables: placed in memory on the stack Or in registers if room available Return value: rax register 4.3.5 Calling Convention Overview Two sets of rules Caller: the function which calls another function From example a few slides back, main Callee: the function which is called by another function From example a few slides back, max 4.3.6 Register Usage One register is used for the return value: rax Six registers are used for parameter passing: rdi, rsi, rdx, rcx, r8, r9 Two registers may be modified by the callee: r10 and r11 If the caller wants to keep those values, they need to be saved by pushing them onto the stack Actually, these two have specific uses depending on the language, so are often not used for this reason. Six registers may not be modified by a subroutine callee: rbx, rbp, r12-r15 If it wants to use them, the subroutine must back them up (onto the stack) and restore them later rsp should almost never be modified directly, as it points to the top of the stack 4.3.7 Varying number of parameters Three ways to have a variable number of parameters Method overloading Foo::bar(int) and Foo::bar(int,float) Default parameters Foo::bar(int x = 3) Variable arguments 4.3.8 Variable number of arguments in C/C++ Following code takes in an arbirary number of parameters Subroutine figures out how many parameters there are You can have a variable number of parameters! #include &lt;cstdarg&gt; #include &lt;iostream&gt; using namespace std; double average (int num, ...) { va_list arguments; double sum = 0; va_start (arguments, num); for ( int x = 0; x &lt; num; x++ ) sum += va_arg (arguments, double); va_end (arguments); return sum / num; } int main() { cout &lt;&lt; average(3, 12.2, 22.3, 4.5) &lt;&lt; endl; cout &lt;&lt; average(5, 3.3, 2.2, 1.1, 5.5, 3.3) &lt;&lt; endl; } 4.3.9 Example: output in C C uses printf in place out cout printf (&quot;A %s, a %s, a %s: %s!\\n&quot;, &quot;man&quot;, &quot;plan&quot;, &quot;canal&quot;, &quot;Panama&quot;); printf (&quot;A percent sign: %%\\n&quot;); printf (&quot;An int: %d\\n&quot;, i); printf (&quot;A float with 2 decimal digits: %.2f\\n&quot;, float_value); The percents with a character are a variable value The variable(s) are after the comma(s) A man, a plan, a canal: Panama! A percent sign: % An int: 3 A float with 2 decimal digits: 3.14 4.4 Caller Rules 4.4.1 Caller Summary Prologue Tasks to take care of BEFORE calling a subroutine Get ready to call subroutine Call the subroutine with the call opcode Epilogue Tasks to complete AFTER subroutine call returns (It is not really called this, but I use this to parallel the equivalent components in the callee convention) 4.4.2 Callee Summary Prologue Tasks to perform BEFORE executing the function body Function body Epilogue Tasks to perform AFTER executing function body, but BEFORE leaving function 4.4.3 Caller Rules/Responsibilities Before calling the function (the prologue) Save registers that might be needed after the call (r10, r11, or param registers if applicable) Place parameters in registers / on stack rdi, rsi, rdx, rcx, r8, r9 Then push extra params onto stack (in reverse order?) Call the function call instruction places return address on stack After the called function returns (the epilogue) Remove parameters from stack (if applicable) Restore saved registers (if applicable) 4.4.4 Prologue Caller-saved registers Registers which the caller must save (push onto the stack) ONLY if it wants the values preserved. r10, r11, and registers used for parameters if need value saved Parameters First six are passed in through registers (see previous slide) params 7-n pushed in reverse order (last parameter first) onto stack Call the subroutine Use the call instruction pushes the return address onto the stack and branches to the subroutine 4.4.5 Epilogue Remove parameters Parameters pushed onto stack must be removed Restore stack to the state before the call What is done with the parameters? Return value If any, held in rax Restore caller-saved registers pop them off the stack (Caller can assume no other registers were modified) 4.4.6 Example This is the code that we will be referencing: long myFunc(long a, long b, long c) { long result; // some code return result; } int main() { long x = 1, z = 3; long retVal = myFunc(x, 123, z); //... return 0; } This is the assembly: ; Want to call a function &quot;myFunc&quot; that takes three ; integer parameters. First parameter is in rax. ; Second parameter is the constant 123. Third ; parameter is in memory location &quot;var&quot; push rdi ; rdi will be a param, so saving it ; long retVal = myFunc(x, 123, z); mov rdi, rax ; put first param in rdi mov rsi, 123 ; put second param in rsi mov rdx, [var] ; put third param in rdx call myFunc ; call the function pop rdi ; restore saved rdi value ; return value of myFunc is now available in rax ; (if there is any return value) 4.5 Callee Rules 4.5.1 Callee Summary (again) Prologue Before executing the function body Function body Epilogue After function body 4.5.2 Caller Rules Example long myFunc(long a, long b, long c) { long result; //some code return result; } int main() { long x = 1, z = 3; long retVal = myFunc(x, 123, z); //... return 0; } 4.5.3 Callee Rules (prologue) Before the body of the function: 1. Allocate local variables - Make space on stack (decrement stack pointer) sub rsp, 8 One of the only times we will do arithmetic directly on rsp (subtract 8 from rsp) Save callee-save registers rbx, rbp, r12-r15 Only need to do this if calee intends to use them, otherwise, no need to save their contents THEN perform body of the function. 4.5.4 Public Service Announcement! Note that you might see code like this: push rbp ; at the start of the callee mov rbp, rsp ... pop rbp ; just before the ending &#39;ret&#39; This is a 32-bit x86 convention that is no longer officially used, but is sometimes seen anyways To omit it, add the -fomit-frame-pointer flag to the compilation line 4.5.5 Callee Rules (epilogue) Return value saved to rax Restore callee-saved registers pop from stack (in reverse order from which pushed) Deallocate local variables add rsp, 8 ; constant here depends on size of locals Return ret 4.5.6 Callee Rules Example (with some more code) long myFunc(long a, long b, long c) { long result; result = c; result += b; return result; } int main() { long x = 1, z = 3; long retVal = myFunc(x, 123, z); //... return 0; } 4.5.7 Callee Rules Example: Making myFunc section .text myFunc: ; prologue sub rsp, 8 ; room for a 64-bit local var (result) push rbx ; save callee-save registers push rbp ; both will be used by myFunc ; subroutine body mov rax, rdi ; param 1 to rax mov rbp, rsi ; param 2 to rbp mov rbx, rdx ; param 3 to rbx mov [rsp+16], rbx ; put rbx into local var add [rsp+16], rbp ; add rbp into local var mov rax, [rsp+16] ; mov contents of local var to rax ; (return value/final result) ; subroutine epilogue pop rbp ; recover callee save registers pop rbx ; REVERSE of when pushed add rsp, 8 ; deallocate local var(s) ret ; pop top value from stack, jump there 4.5.8 Callee Program: Step by Step sub rsp, 8 rsp moves down by 8 bytes in size for local variable push rbx Whatever is in rbx is going to be pushed onto the stack rsp moves down again by 8 bytes push rbp Whatever is in rbp is pushed onto stack rsp lowers by 8 bytes mov rax, rdi rdi was already assigned to 1 Takes this value and copies it into rax rax is now value 1 mov rbp, rsi rsi was also already assigned as 123 This gets copied into rbp rbp is now 123 mov rbx, rdx rdx was already assigned to 3 This gets copied into rbx rbx is now 3 mov [rsp+16], rbx rbx gets moved to rsp+16 (lower on the stack) add [rsp+16], rbp Add rbp (123) to this local variable (3) = 126 mov rax, [rsp+16] Copy local value (126) into rax rax is now 126 pop rbp Whatever value was in rbp is restored pop rbx Whatever value was in rbx is restored add rsp, 8 Restore ret Return value 4.5.9 That subroutine, again The previous assembly method could be written as mov rax, rdx add rax, rsi ret It was moreso to show how the calling convention works 4.6 Activation Records Every time a subroutine is called, a number of things are pushed onto the stack Registers Parameters Local variables Return address All of this is called the activation record 4.6.1 Memory Management The binary program takes up a fixed amount of space The size of the file There are two types of memory that need to be handled Dynamic memory (via new, malloc(), etc) This is stored on the heap Static memory (on the stack) This is where the activation records are kept The binary program starts at the beginning of the \\(2^{32} = 4\\) Gb of memory The heap starts right after this Ex. Address 10,000 if a 10 kb binary file The stack starts at the end of this 4 Gb of memory, and grows backward As a program progresses, they both grow toward the middle 4.6.2 Consider this subroutine… void security_hole() { char buffer[12]; scanf (&quot;%s&quot;, buffer); // how C handles input } The stack would look like this (sizes in parenthesis, addresses increase towards the top, stack grows downward): - ret addr (8) - buffer (12) - rdi (8) - rsi (8) Now, what if we get something that is bigger than 12 characters to put into the buffer? - Ex. 20 bytes - Fill 12 bytes to buffer - Fill 8 remaining bytes to return address (ret addr) - This is called a buffer overflow - Overflowing the buffer - “Smashing the stack” - Overwriting things on the stack - Now the return address is overwritten with some characters - Leads to: - At best? a segfault - At worst? a security hole… 4.6.3 Buffer Overflow Attack When you read in a string/etc that goes beyond the size of the buffer “Buffer overflow” Make this string the malicious code You overwrite the return address And set it to the buffer address Now, it returns into the buffer and runs the buffer code when closing the subroutine 4.7 x86 Examples 4.7.1 A note about x86 compatibility nasm is the assembler we are using x86 code in slides are either “Generic” x86 examples that could work in nasm if put into a full program Like the C++ snippets Or output from clang++ -S Doesn’t work with nasm but works with clang++ 4.7.2 test_abs.cpp C++ code: #include &lt;iostream&gt; using namespace std; extern &quot;C&quot; int absolute_value(int x); long absolute_value(long x) { if (x&lt;0) // if x is negative x = -x; // negate x return x; // return x } int main() { long theValue=0; cout &lt;&lt; &quot;Enter a value: &quot; &lt;&lt; endl; cin &gt;&gt; theValue; long theResult = absolute_value(theValue); cout &lt;&lt; &quot;The result is: &quot; &lt;&lt; theResult &lt;&lt; endl; return 0; } x86 Assembly code for absolute_value: ; prologue, nothing here because no saved ; registers and no local vars ; procedure body mov rax, rdi ; rax &lt;- x cmp rax, 0 ; x == 0 ? jge end_of_proc ; if pos goto end neg rax ; negate x end_of_proc: ; Standard epilogue, no local vars and no saved regs ret 4.7.3 Generating assembly with clang++ Using the -S flag, we can generate assembly from C/C++ code Note that the format is a bit different Register specification, dest/source order, etc. So we need to use the following command: clang++ -m64 -S test_abs.cpp -o test_abs-non-intel.s You’ll see why it has ‘non-intel’ in the name shortly… Actually, we use even more flags… 4.7.4 clang++’s assembly for absolute_value Note that the source/destination order is reversed (moving the first into the second, instead of second into the first like we are used to), and a lot of other differences. This is the AT&amp;T syntax. We aren’t using this, we are using the Intel syntax. absolute_value: movq %rdi, -8(%rsp) cmpq $0, -8(%rsp) jge .LBB1_2 xorl %eax, %eax movl %eax, %ecx subq -8(%rsp), %rcx movq %rcx, -8(%rsp) .LBB1_2: movq -8(%rsp), %rax retq 4.7.5 Assembly diffeerences There are a lot of differences between the assembly we’ve seen and what clang++ -O2 -S -fomit-frame-pointer produces The source / destination order is reversed movq, sarq, xorq etc. the ‘q’ part means ‘quadword’ (i.e. 64 bits, so the assembler doesn’t have to guess) $0 is the constant 0 Register names begin with a percent sign -8(%rsp) is [rsp-8] 4.7.6 Assembly syntax “flavors” There are two primary syntax “flavors” of x86 assembly The difference is solely in the formatting; all assembly commands can be written in either format You can see more details about the differences here Flavors: Intel syntax: what we will use, and what nasm uses AT&amp;T syntax: what clang++ -S uses by default To make clang++ use the Intel syntax, you need to add the -mllvm --x86-asm-syntax=intel flags clang++ -m64 -mllvm --x86-asm-syntax=intel -S -fomit-frame-pointer test_abs.cpp -o test_abs.s 4.7.7 clang++’s assembly for absolute_value This is with the -mllvm --x86-asm-syntax=intel flags. Note that this looks longer than what we made but is a lot similar to what we are programming like than the last output (since this is Intel vs the last being AT&amp;T). absolute_value: mov qword ptr [rsp - 8], rdi cmp qword ptr [rsp - 8], 0 jge .LBB1_2 xor eax, eax mov ecx, eax sub rcx, qword ptr [rsp - 8] mov qword ptr [rsp - 8], rcx .LBB1_2: mov rax, qword ptr [rsp - 8] ret 4.7.8 test_abs_int.cpp This is the same as before, but instead of long it uses int. Refer to the code above in the test_abs.cpp part. In assembly, it would look like this when compiled with clang++: absolute_value: mov dword ptr [rsp - 4], edi cmp dword ptr [rsp - 4], 0 jge .LBB1_2 xor eax, eax sub eax, dword ptr [rsp - 4] mov dword ptr [rsp - 4], eax .LBB1_2: mov eax, dword ptr [rsp - 4] ret Because ints are smaller than longs, note that it ofsets [rsp-4] rather than [rsp-8]. Also, it uses eax rather than rax. 4.7.9 test_abs_c.c This is a C program. We are also back to using longs. #include &lt;stdio.h&gt; long absolute_value(long x) { if (x&lt;0) // if x is negative x = -x; // negate x return x; // return x } int main() { long theValue=0; printf (&quot;Enter a value: \\n&quot;); scanf (&quot;%ld&quot;, &amp;theValue); long theResult = absolute_value(theValue); printf (&quot;The result is: %ld\\n&quot;, theResult); return 0; } The reason that we are using C for this example is because cout and cin are very ugly in assembly. They don’t exist in C, so it is less ugly! We can look at the compiled versions of the two code snippets to see the differences: - C++: test_abs.s - C: test_abs_c.s 4.7.10 int max(int x, int y) Very inefficient, intentionally so. We don’t want to make it efficient yet for the sake of learning int max(int x, int y) { int theMax; if (x &gt; y) // if x &gt; y then x is max theMax = x; else // else y is the max theMax = y; return theMax; // return the max } 4.7.11 x86 code for max() This is moving edi and esi into local variables. Whichever one is greater is moved to a local variable [rsp-12], then returned using eax. max: mov dword ptr [rsp - 4], edi mov dword ptr [rsp - 8], esi mov esi, dword ptr [rsp - 4] cmp esi, dword ptr [rsp - 8] jle .LBB1_2 mov eax, dword ptr [rsp - 4] mov dword ptr [rsp - 12], eax jmp .LBB1_3 .LBB1_2: mov eax, dword ptr [rsp - 8] mov dword ptr [rsp - 12], eax .LBB1_3: mov eax, dword ptr [rsp - 12] ret 4.7.12 x86 code for max() using -O2 Now, we can compile with -O2 to make the program more efficient by condensing it as much as possible. max: cmp edi, esi cmovge esi, edi mov eax, esi ret cmovg (conditional move if greater than) will move the greater value into the first parameter cmovge does the move greater than or equal to value This removes the needs for jumps because jumps are very inefficient You still need to write good code, but this makes things faster. 4.7.13 C-string comparison compare_string() bool compare_string (const char *theStr1, const char *theStr2) { // while *theStr1 is not NULL terminator // and the current corresponding bytes are equal while( (*theStr1 != NULL) &amp;&amp; (*theStr1 == *theStr2) ) { theStr1++; // increment the pointers to theStr2++; // the next char / byte } return (*theStr1==*theStr2); } #include &lt;iostream&gt; #include &lt;string&gt; using namespace std; extern &quot;C&quot; bool compare_string(const char* theStr1, const char* theStr2); // code for compare_string here int main() { string theValue1, theValue2; cout &lt;&lt; &quot;Enter string 1: &quot; &lt;&lt; endl; cin &gt;&gt; theValue1; cout &lt;&lt; &quot;Enter string 2: &quot; &lt;&lt; endl; cin &gt;&gt; theValue2; bool theResult = compare_string(theValue1.c_str(), theValue2.c_str()); cout &lt;&lt; &quot;The result is: &quot; &lt;&lt; theResult &lt;&lt; endl; return 0; } We use extern... because we want to be able to call the method its defined name in the assembly 4.7.14 x86 for compare_string() compare_string: mov al, byte ptr [rdi] test al, al je .LBB0_4 inc rdi .LBB0_2: movzx ecx, byte ptr [rsi] movzx edx, al cmp edx, ecx jne .LBB0_5 inc rsi mov al, byte ptr [rdi] inc rdi test al, al jne .LBB0_2 ... zx stands for “zero extend” Loads a value in and fills everything else as 0s 4.7.15 int fact() factorial long fact(unsigned int n) { if ( n==0 ) return 1; return n * fact(n-1); } Note the parameter is unsigned int (32 bit), but the return type is long (64 bit) 4.7.16 x86 assembly for fact() fact: sub rsp, 24 mov dword ptr [rsp + 12], edi cmp dword ptr [rsp + 12], 0 jne .LBB1_2 mov qword ptr [rsp + 16], 1 jmp .LBB1_3 .LBB1_2: mov eax, dword ptr [rsp + 12] mov ecx, eax mov eax, dword ptr [rsp + 12] sub eax, 1 mov edi, eax mov qword ptr [rsp], rcx call fact mov rcx, qword ptr [rsp] imul rcx, rax mov qword ptr [rsp + 16], rcx .LBB1_3: mov rax, qword ptr [rsp + 16] add rsp, 24 ret We are going to be using push and pop They move a lot of values because they’re all already assigned from free space This is faster but more complicated Make sure to push whatever much you pop. Modifications to the stack need to equal each other 4.7.17 An advantage of little-endian In little-endian, the order of the bytes are flipped from big-endian The four byte value and the one byte value of a word (ex. 0x680abcde) have the same value. In big-endian, they don’t and you need to shift over a lot to get the same value. 4.7.18 RISC versus CISC RISC Reduced instruction set computer Fewer and simpler instructions (maybe 50 or so) Less chip complexity means they can run fast CISC Complex instruction set computer More and more complex instructions (300-400 or so) More chip complexity means harder to make run fast Hotter chips Intel is CISC "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
